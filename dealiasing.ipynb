{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362cd177-4558-4761-8fc2-ea292edb9e45",
   "metadata": {},
   "source": [
    "I want to de-alias all VAD files including the ones in motion. To do this I will import functions from Alex's VADS_2 file, which contains functions that perform all desried actions. -- Julia Buhrman 02/20/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548e0af4-a73e-4a2c-8ba2-e53a0631530e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## You are using the Python ARM Radar Toolkit (Py-ART), an open source\n",
      "## library for working with weather radar data. Py-ART is partly\n",
      "## supported by the U.S. Department of Energy as part of the Atmospheric\n",
      "## Radiation Measurement (ARM) Climate Research Facility, an Office of\n",
      "## Science user facility.\n",
      "##\n",
      "## If you use this software to prepare a publication, please cite:\n",
      "##\n",
      "##     JJ Helmus and SM Collis, JORS 2016, doi: 10.5334/jors.119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#imports all needed modules\n",
    "\n",
    "import pyart\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "# from radarcalc import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import metpy.calc as mpcalc\n",
    "import metpy\n",
    "import metpy.plots\n",
    "from metpy.units import units\n",
    "import cartopy.crs as ccrs\n",
    "import gc\n",
    "from astropy.convolution import convolve\n",
    "from boto.s3.connection import S3Connection\n",
    "import tempfile\n",
    "import copy\n",
    "import matplotlib\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "# import from Alex's VAD_2 notebook which I changed to a .py file (so we can import stuff from it)\n",
    "#from VADS_2_Copy import dealias_Ka, vehicle_correction_vad <== MAKES JUPYTER LAB BUFFER IDK WHY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e6ed2-8e24-41f2-a93d-6d00b41873dd",
   "metadata": {},
   "source": [
    "I guess just copy and paste the functions from VADS_2 :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994bc430-e71a-48d7-b54f-eb37520d34ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyart\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "# from radarcalc import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import metpy.calc as mpcalc\n",
    "import metpy\n",
    "import metpy.plots\n",
    "from metpy.units import units\n",
    "import cartopy.crs as ccrs\n",
    "import gc\n",
    "from astropy.convolution import convolve\n",
    "from boto.s3.connection import S3Connection\n",
    "import tempfile\n",
    "import copy\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "def dealias_Ka(radar,PPIGC_flag=False):\n",
    "    '''\n",
    "    This function aims to take care of all the nitty gritty customizations of pyarts dealiasing specifically for TTUKa radar data.\n",
    "\n",
    "    Parameter: radar (pyart object), PPIGC_flag (boolean *kwarg)\n",
    "    Returns: radar (pyart object) with corrected_velocity field and velocity_texture field added on\n",
    "\n",
    "    How it works:\n",
    "    1. Calculates velocity texture and creates a histogram based on all of the magnitudes of textures at each bin for the entire volume\n",
    "    2. loops through each indivual sweep\n",
    "    3. infinite loop to iterate between the minimum amount of texture between textures 1 and 6 and the maximum amount of texture in the histogram, with 0.5 m/s steps\n",
    "    4. Each iteration, create a gatefilter that filters the bins above that texture value and dealias it with the gatefilter to ignore the high textured regions\n",
    "    5. If the maximum texture is reached before breaking, set the gatefilter to mask textures above 12 m/s\n",
    "    6. if the scan is an RHI, run a 2 pass variance filter along each ray.\n",
    "        This convolves a 71 sized boxcar with the data, and if the difference between the point the boxcar is centered on and the mean of the boxcar is greater than the nyquist, then add/subtract 2*nyquist to that point\n",
    "    7. Then, take the difference between the mean of the bottom 4 rays of the dealiased velocity in the RHI and the bottom 4 rays of the aliased velocity\n",
    "    8. If the absolute difference is larger than the nyquist, the either subtract 2* nyquist or add 2*nyquist to the entire sweep and break the loop\n",
    "    9. If the absolute difference is less than the nyquist, no fixes need to be applied, break the loop\n",
    "    10. If the scan is a PPI, do steps 8 and 9, but skip steps 6 and 7\n",
    "    11. Outside the infinite loop, add the option of the PPIGC_flag, a boolean flag that will help maintain ground clutter in PPIs at 0 m/s\n",
    "        This works by identifying regions of very low spectrum width (<0.1 m/s), and setting the velocity in those regions = 0. Please note, this may introduce artificial speckles of 0 in real data, where spectrum width is noisy\n",
    "    12. Apply the alias fix algorithm which convolves a boxcar of specified boxcar of size 9 with the data, and if the variance between the middle pixel of interest and the mean is greater than the nyquist, then flip it back over\n",
    "    12. At the end of each sweep, assign the data from that processed sweep into a dictionary, then add the dictionary as the corrected_velocity field.\n",
    "    \n",
    "    The aforementioned method is FAR from perfect, but is as robust as I can do currently. One thing to improve this though is to use the UNRAVEL algorithm: https://github.com/vlouf/dealias\n",
    "    The UNRAVEL algorithm shows remarkable error characteristics compared to \"competitors\", possibly at a time cost, which isn't a HUGE deal for us. Downside is it may not work for our \"volumes\" since they are temporally uncorrelated and are not full volumes through the atmosphere\n",
    "    '''\n",
    "    \n",
    "    vel_texture = pyart.retrieve.calculate_velocity_texture(radar, vel_field='velocity', wind_size=3)\n",
    "    radar.add_field('velocity_texture', vel_texture, replace_existing=True)\n",
    "    hist, bins = np.histogram(radar.fields['velocity_texture']['data'][~np.isnan(radar.fields['velocity_texture']['data'])], bins=150)\n",
    "    bins = (bins[1:]+bins[:-1])/2.0\n",
    "    gatefilter = pyart.filters.GateFilter(radar)\n",
    "    velocity_dealiased = pyart.correct.dealias_region_based(radar, vel_field='velocity', nyquist_vel=radar.instrument_parameters['nyquist_velocity']['data'][0], centered=True) #standin, data will be replaced\n",
    "\n",
    "    for swp_id in range(radar.nsweeps):\n",
    "        #get indices from beginning and ending of sweep\n",
    "        sw_start = radar.sweep_start_ray_index['data'][swp_id]\n",
    "        sw_end = radar.sweep_end_ray_index['data'][swp_id]+1\n",
    "\n",
    "        counter = 0\n",
    "        while True: #do an infinite loop and either break it when the data is unfolded correctly or when the max texture is reached\n",
    "            #if the bin with the lowest count between textures 1 and 6 + i*0.5 is less than the maximum amount of bins\n",
    "            if bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]]+counter*0.5 < np.amax(bins):\n",
    "                gatefilter.exclude_above('velocity_texture', bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]]+counter*0.5)\n",
    "                nyq = radar.instrument_parameters['nyquist_velocity']['data'][0]\n",
    "                vede = pyart.correct.dealias_region_based(radar, vel_field='velocity', nyquist_vel=nyq,\n",
    "                                                                        centered=True, gatefilter=gatefilter)\n",
    "            else:\n",
    "                gatefilter.exclude_above('velocity_texture', 12)#bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]])\n",
    "                nyq = radar.instrument_parameters['nyquist_velocity']['data'][0]\n",
    "                vede = pyart.correct.dealias_region_based(radar, vel_field='velocity', nyquist_vel=nyq,\n",
    "                                                                        centered=True, gatefilter=gatefilter)\n",
    "\n",
    "\n",
    "            np.ma.set_fill_value(vede['data'], np.nan)\n",
    "            #extract mask so we can apply the correct gatefilters on later\n",
    "            mask=np.ma.getmask(vede['data'])\n",
    "\n",
    "            #apply mask to velocity field and fix the small blips from dealiasing\n",
    "            if radar.scan_type == 'rhi':\n",
    "                #pass 1 of variance filtering along the ray.\n",
    "                #Convolves a 71 sized boxcar with the data, and if the difference between the point the boxcar is centered on and the mean of the boxcar is greater than the nyquist, then add/subtract 2*nyquist to that point\n",
    "                vel = vede['data'].filled()\n",
    "\n",
    "                for sw in range(np.shape(vel)[0]):\n",
    "                    mean = convolve(vel[sw,:],np.ones(71))\n",
    "                    var = vel[sw,:]-mean\n",
    "                    high_idx = var > nyq\n",
    "                    low_idx = var < -nyq\n",
    "                    vel[sw,:][high_idx] = vel[sw,:][high_idx] - 2*nyq\n",
    "                    vel[sw,:][low_idx] = vel[sw,:][low_idx] + 2*nyq\n",
    "                vede['data']=np.ma.masked_array(vel,mask=mask,fill_value=np.nan)\n",
    "\n",
    "                #pass 2 of variance filtering along the ray. In case there are errant folds than need to be folded back\n",
    "                vel = vede['data'].filled()\n",
    "                for sw in range(np.shape(vel)[0]):\n",
    "                    mean = convolve(vel[sw,:],np.ones(71))\n",
    "                    var = vel[sw,:]-mean\n",
    "                    high_idx = var > nyq\n",
    "                    low_idx = var < -nyq\n",
    "                    vel[sw,:][high_idx] = vel[sw,:][high_idx] - 2*nyq\n",
    "                    vel[sw,:][low_idx] = vel[sw,:][low_idx] + 2*nyq\n",
    "                vede['data']=np.ma.masked_array(vel,mask=mask,fill_value=np.nan)\n",
    "\n",
    "                #find means of the bottom 4 rays of the RHI(should be close to 0) and compare the dealiased velocities to the aliased velocities\n",
    "                np.ma.set_fill_value(radar.fields['velocity']['data'], np.nan)\n",
    "                meanvelal = np.mean(radar.fields['velocity']['data'][sw_start:sw_start+4,:].filled()[~np.isnan(radar.fields['velocity']['data'][sw_start:sw_start+4,:].filled())])\n",
    "                meanveldeal = np.mean(vede['data'][sw_start:sw_start+4,:].filled()[~np.isnan(vede['data'][sw_start:sw_start+4,:].filled())])\n",
    "                if np.abs(meanvelal-meanveldeal) < nyq: #nyq is an arbitrary threshold and should be tuned\n",
    "                    break\n",
    "                if bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]]+counter*0.5 < np.amax(bins):\n",
    "                    if (meanvelal-meanveldeal) > 0:\n",
    "                        vede['data'][sw_start:sw_end,:] += 2*nyq\n",
    "                    else:\n",
    "                        vede['data'][sw_start:sw_end,:] -= 2*nyq\n",
    "                    break\n",
    "            if radar.scan_type == 'ppi':\n",
    "                np.ma.set_fill_value(radar.fields['velocity']['data'], np.nan)\n",
    "                meanvelal = np.mean(radar.fields['velocity']['data'][sw_start:sw_end,:].filled()[~np.isnan(radar.fields['velocity']['data'][sw_start:sw_end,:].filled())])\n",
    "                meanveldeal = np.mean(vede['data'][sw_start:sw_end,:].filled()[~np.isnan(vede['data'][sw_start:sw_end,:].filled())])\n",
    "                if np.abs(meanvelal-meanveldeal) < nyq: #nyq is an arbitrary threshold and should be tuned\n",
    "                    break\n",
    "                if bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]]+counter*0.5 < np.amax(bins):\n",
    "                    if (meanvelal-meanveldeal) > 0:\n",
    "                        vede['data'][sw_start:sw_end,:] += 2*nyq\n",
    "                    else:\n",
    "                        vede['data'][sw_start:sw_end,:] -= 2*nyq\n",
    "                    break\n",
    "            counter+=1\n",
    "            \n",
    "        #put alias fix inside here instead of calling it to make it more portable\n",
    "        delta=3\n",
    "        mean = convolve(vede['data'][sw_start:sw_end,:],np.ones((delta,delta))/delta**2.)\n",
    "        mean[0,:] = vede['data'][sw_start:sw_end,:][0,:]\n",
    "        mean[-1,:] = vede['data'][sw_start:sw_end,:][-1,:]\n",
    "        var = vede['data'][sw_start:sw_end,:] - mean\n",
    "\n",
    "        high_idx = np.logical_and(var > nyq, var < 4*nyq)\n",
    "        low_idx = np.logical_and(var < -nyq, var > -4*nyq)\n",
    "\n",
    "        vede['data'][sw_start:sw_end,:][high_idx] = vede['data'][sw_start:sw_end,:][high_idx] - 2*nyq\n",
    "        vede['data'][sw_start:sw_end,:][low_idx] = vede['data'][sw_start:sw_end,:][low_idx] + 2*nyq\n",
    "\n",
    "        #corrects ground clutter by arbitrarily setting the velocity equal to 0 where spectrum width is less than 0.075 m/s\n",
    "        if PPIGC_flag == True:\n",
    "            if radar.scan_type == 'ppi':\n",
    "                sw = radar.fields['spectrum_width']['data'][sw_start:sw_end,:].filled()\n",
    "                vel = radar.fields['velocity']['data'][sw_start:sw_end,:].filled()\n",
    "                mask = sw<0.1\n",
    "                vede['data'][sw_start:sw_end,:] = np.where(~mask,vede['data'][sw_start:sw_end,:],0)\n",
    "\n",
    "        velocity_dealiased['data'][sw_start:sw_end,:] = vede['data'][sw_start:sw_end,:]\n",
    "        velocity_dealiased['data'][sw_start:sw_end,:] = alias_fix(velocity_dealiased['data'][sw_start:sw_end,:],nyq,delta=9)\n",
    "    radar.add_field('corrected_velocity', velocity_dealiased, replace_existing=True)\n",
    "\n",
    "    return radar\n",
    "\n",
    "def alias_fix(vel,nyq,delta=3):\n",
    "    '''\n",
    "    !!!!!!!!!!!!!!!!!!\n",
    "    Removes dealiasing errors around the periphery of a folded region\n",
    "\n",
    "    Parameters: velocity array (array), nyquist velocity (number), size of window (int, must be odd, unity is no change)\n",
    "    Returns: cleaned velocity array (array)\n",
    "    '''\n",
    "    mean = convolve(vel,np.ones((delta,delta))/delta**2.)\n",
    "    mean[0,:] = vel[0,:]\n",
    "    mean[-1,:] = vel[-1,:]\n",
    "    var = vel - mean\n",
    "\n",
    "    high_idx = np.logical_and(var > nyq, var < 4*nyq)\n",
    "    low_idx = np.logical_and(var < -nyq, var > -4*nyq)\n",
    "\n",
    "    vel[high_idx] = vel[high_idx] - 2*nyq\n",
    "    vel[low_idx] = vel[low_idx] + 2*nyq\n",
    "\n",
    "    return vel\n",
    "\n",
    "def get_radar_from_aws(site, datetime_t, datetime_te):\n",
    "    \"\"\"\n",
    "    Get the closest volume of NEXRAD data to a particular datetime.\n",
    "    Parameters\n",
    "    ----------\n",
    "    site : string\n",
    "        four letter radar designation\n",
    "    datetime_t : datetime\n",
    "        desired date time\n",
    "    Returns\n",
    "    -------\n",
    "    radar : Py-ART Radar Object\n",
    "        Radar closest to the queried datetime\n",
    "    \"\"\"\n",
    "\n",
    "    # First create the query string for the bucket knowing\n",
    "    # how NOAA and AWS store the data\n",
    "    my_pref = datetime_t.strftime('%Y/%m/%d/') + site\n",
    "\n",
    "    # Connect to the bucket\n",
    "    conn = S3Connection(anon = True)\n",
    "    bucket = conn.get_bucket('noaa-nexrad-level2')\n",
    "\n",
    "    # Get a list of files\n",
    "    bucket_list = list(bucket.list(prefix = my_pref))\n",
    "\n",
    "    # we are going to create a list of keys and datetimes to allow easy searching\n",
    "    keys = []\n",
    "    datetimes = []\n",
    "\n",
    "    # populate the list\n",
    "    for i in range(len(bucket_list)):\n",
    "        this_str = str(bucket_list[i].key)\n",
    "        if 'gz' in this_str:\n",
    "            endme = this_str[-22:-4]\n",
    "            fmt = '%Y%m%d_%H%M%S_V0'\n",
    "            dt = datetime.strptime(endme, fmt)\n",
    "            datetimes.append(dt)\n",
    "            keys.append(bucket_list[i])\n",
    "\n",
    "        if this_str[-3::] == 'V06':\n",
    "            endme = this_str[-19::]\n",
    "            fmt = '%Y%m%d_%H%M%S_V06'\n",
    "            dt = datetime.strptime(endme, fmt)\n",
    "            datetimes.append(dt)\n",
    "            keys.append(bucket_list[i])\n",
    "\n",
    "    # find the closest available radar to your datetime\n",
    "    closest_datetime_b = _nearestDate(datetimes, datetime_t)\n",
    "    closest_datetime_e = _nearestDate(datetimes, datetime_te)\n",
    "\n",
    "    index_b = datetimes.index(closest_datetime_b)\n",
    "    index_e = datetimes.index(closest_datetime_e)\n",
    "\n",
    "    radar_namelist = keys[index_b:index_e+1]\n",
    "    radar_list=[]\n",
    "    for i in range(np.shape(radar_namelist)[0]):\n",
    "        localfile = tempfile.NamedTemporaryFile()\n",
    "        radar_namelist[i].get_contents_to_filename(localfile.name)\n",
    "        radar_list.append(pyart.io.read(localfile.name))\n",
    "    return radar_namelist,radar_list\n",
    "\n",
    "def getLocation(lat1, lon1, brng, distancekm):\n",
    "    lat1 = lat1 * np.pi / 180.0\n",
    "    lon1 = lon1 * np.pi / 180.0\n",
    "    #earth radius\n",
    "    R = 6378.1\n",
    "    #R = ~ 3959 MilesR = 3959\n",
    "    bearing = (brng / 90.)* np.pi / 2.\n",
    "\n",
    "    lat2 = np.arcsin(np.sin(lat1) * np.cos(distancekm/R) + np.cos(lat1) * np.sin(distancekm/R) * np.cos(bearing))\n",
    "    lon2 = lon1 + np.arctan2(np.sin(bearing)*np.sin(distancekm/R)*np.cos(lat1),np.cos(distancekm/R)-np.sin(lat1)*np.sin(lat2))\n",
    "    lon2 = 180.0 * lon2 / np.pi\n",
    "    lat2 = 180.0 * lat2 / np.pi\n",
    "    return lat2, lon2\n",
    "\n",
    "def _nearestDate(dates, pivot):\n",
    "    return min(dates, key=lambda x: abs(x - pivot))\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    '''\n",
    "    Function to find index of the array in which the value is closest to\n",
    "\n",
    "    Parameters: array (array), value (number)\n",
    "    Returns: index (int)\n",
    "\n",
    "    Example: xind = CM1calc.find_nearest(x,5)\n",
    "    '''\n",
    "\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def vehicle_correction_vad(radar,df):\n",
    "    '''\n",
    "    Function that creates a 'vad_corrected_velocity' field that can be used for vad calculations, \n",
    "    but should be general enough to use for stationary VADs as well as moving PPIs. \n",
    "    Other than adding the new field, the radar times are smoothly interpolated and the azimuths are \n",
    "    corrected via the GPS pandas dataframe.\n",
    "    \n",
    "    Parameters: pyart radar object (object), pandas dataframe of appropriate radarGPS file (dataframe)\n",
    "    Returns: pyart radar object (object), speed (float), speed variance (float), bearing (float), bearing variance (float), \n",
    "             latitude (float), latitude variance (float), longitude (float), longitude variance (float)\n",
    "    \n",
    "    Example: radar, velmean, velvar, bearmean, bearvar, latmean, latvar, lonmean, lonvar = vehicle_correction_vad(radar,df)\n",
    "\n",
    "    p.s. only works if the velocity is already dealiased and there is a 'corrected_velocity' field\n",
    "         also only works if a single sweep is extracted, example: radar = radar.extract_sweeps([0])\n",
    "    '''\n",
    "    \n",
    "    #orders the time to increase monotonically instead of having a massive step jump in the middle\n",
    "    roll_mag = (np.argmax(np.abs(np.gradient(radar.time['data'])))+1)\n",
    "    times = np.roll(radar.time['data'],-roll_mag) \n",
    "    \n",
    "    #a complicated way to create linear increasing times (instead of steps) that start at 0 seconds after the time datum and increase to the middle of the second max time plateau (if confused, plotting it is helpful)\n",
    "    #from now on, we are going to assume ray_times is the fractional seconds after the time datum the ray is gathered, and we need to roll it back to match with the rest of the data\n",
    "    ray_times = np.roll(np.arange(0,((np.unique(times)[-2])/(find_nearest(times,np.unique(times)[-2])+int(np.sum(radar.time['data']==np.unique(times)[-2])/2)))*len(times)+1e-11,((np.unique(times)[-2])/(find_nearest(times,np.unique(times)[-2])+int(np.sum(radar.time['data']==np.unique(times)[-2])/2)))),roll_mag)\n",
    "\n",
    "    radar.time['data']=ray_times\n",
    "\n",
    "    print([datetime.strptime(d,'%d%m%y%H%M%S') for d in df['ddmmyy']+[f'{h:06}' for h in df['hhmmss[UTC]'].astype(int)]][19660:19680])\n",
    "    print(datetime.strptime(radar.time['units'],'seconds since %Y-%m-%dT%H:%M:%SZ'))\n",
    "    \n",
    "    df['datetime'] = [datetime.strptime(d,'%d%m%y%H%M%S') for d in df['ddmmyy']+df['hhmmss[UTC]']]\n",
    "    beginscanindex = df.loc[df['datetime'] == datetime.strptime(radar.time['units'],'seconds since %Y-%m-%dT%H:%M:%SZ')].index\n",
    "    endscanindex = df.loc[df['datetime'] == datetime.strptime(radar.time['units'],'seconds since %Y-%m-%dT%H:%M:%SZ')].index+np.ceil(np.amax(ray_times))+1\n",
    "    dfscan = df.iloc[beginscanindex[0].astype(int):endscanindex[0].astype(int)]\n",
    "    dfscan = dfscan.astype({'Bearing[degrees]': 'float'})\n",
    "    dfscan = dfscan.astype({'Velocity[knots]': 'float'})\n",
    "\n",
    "    ray_bearings = np.interp(ray_times,np.arange(len(dfscan)),dfscan['Bearing[degrees]'])\n",
    "    ray_speeds = np.interp(ray_times,np.arange(len(dfscan)),dfscan['Velocity[knots]'])\n",
    "\n",
    "    print('velocity [kts]',dfscan['Velocity[knots]'].mean(),'+-',dfscan['Velocity[knots]'].var())\n",
    "    speed = dfscan['Velocity[knots]'].mean()\n",
    "    print('bearing',dfscan['Bearing[degrees]'].mean(),'+-',dfscan['Bearing[degrees]'].var())\n",
    "    bearing = dfscan['Bearing[degrees]'].mean()\n",
    "    print('latitude',dfscan['Latitude'].astype(float).mean(),'+-',dfscan['Latitude'].astype(float).var())\n",
    "    lat = dfscan['Latitude'].astype(float).mean()\n",
    "    print('longitude',dfscan['Longitude'].astype(float).mean(),'+-',dfscan['Longitude'].astype(float).var())\n",
    "    lon = dfscan['Longitude'].astype(float).mean()\n",
    "\n",
    "    radar.azimuth['data'] += ray_bearings[:-1] #bearing\n",
    "\n",
    "    rad_vel = copy.deepcopy(radar.fields['corrected_velocity'])\n",
    "\n",
    "    rad_vel['data']+=(np.cos(np.deg2rad(radar.azimuth['data']-ray_bearings[:-1]))*(ray_speeds[:-1]/1.94384)*np.cos(np.deg2rad(radar.fixed_angle['data'][0])))[:,np.newaxis]\n",
    "\n",
    "    #fix mask, remove points very close to radar as well as the very last bin, more often than not, = bad data\n",
    "    rad_vel['data'].mask[:,:5] = True\n",
    "    rad_vel['data'].mask[:,-1] = True\n",
    "    radar.add_field('vad_corrected_velocity', rad_vel, replace_existing=True)\n",
    "\n",
    "    return radar, dfscan['Velocity[knots]'].mean(),dfscan['Velocity[knots]'].var(),dfscan['Bearing[degrees]'].mean(),dfscan['Bearing[degrees]'].var(),dfscan['Latitude'].astype(float).mean(),dfscan['Latitude'].astype(float).var(),dfscan['Longitude'].astype(float).mean(),dfscan['Longitude'].astype(float).var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e58eb0c-03f7-4840-960a-77bb6845af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in the files\n",
    "# gps data for first ka\n",
    "ka1gps = pd.read_csv('/Users/juliabman/Desktop/research2024/GPS_Ka1_20220523.txt')\n",
    "# scan data for first ka\n",
    "ka1scan = pd.read_csv('/Users/juliabman/Desktop/research2024/20220523_Ka1_scan_log.txt', dtype=str)\n",
    "\n",
    "# dealiased vad data created from VADS.ipynb for ka 1, but not bearing corrected\n",
    "vad_fileska1 = sorted(glob.glob('/Users/juliabman/Desktop/research2024/product_raw_ka1/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd376a74-d61f-46bf-a383-e70d9b0fcca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dealiased_data = sorted(glob.glob('/Users/juliabman/Desktop/dealiased_data/*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d31b5-420b-42f4-aae3-45fa81ded613",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(vad_fileska1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9162de0-b41c-4d86-9c1c-8ac4e119c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496ddfc8-109c-4a67-b69e-d3caffeb37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for ka 2\n",
    "\n",
    "ka2gps = pd.read_csv('/Users/juliabman/Desktop/research2024/GPS_Ka2_20220523.txt')\n",
    "# scan data for first ka\n",
    "ka2scan = pd.read_csv('/Users/juliabman/Desktop/research2024/20220523_Ka2_scan_log.txt', dtype=str)                    \n",
    "\n",
    "# dealiased vad data created from VADS.ipynb for ka 2, but not bearing corrected\n",
    "vad_fileska2 = sorted(glob.glob('/Users/juliabman/Desktop/research2024/product_raw_ka2/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b59290-b41b-491b-a6fd-bc720de7fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(vad_fileska2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75872ec-9b74-412b-9005-91310c6adade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Datetime to pandas so it will be useable later\n",
    "ka1scan['Datetime'] = pd.to_datetime(ka1scan['Datetime'])\n",
    "ka2scan['Datetime'] = pd.to_datetime(ka2scan['Datetime']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd371ac-5c75-48b0-97c6-a0c0a8afe63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ddmmyy to string since thats what the code requires\n",
    "\n",
    "ka1gps['ddmmyy'] = ka1gps['ddmmyy'].astype(str)\n",
    "ka1gps['hhmmss[UTC]'] = ka1gps['hhmmss[UTC]'].astype(str)\n",
    "\n",
    "ka2gps['ddmmyy'] = ka2gps['ddmmyy'].astype(str)\n",
    "ka2gps['hhmmss[UTC]'] = ka2gps['hhmmss[UTC]'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11591fbd-856f-4957-ab79-b585bda5abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ka1gps.datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b1f153-30bf-4771-956f-7f4b58ad1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_one = vad_fileska1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc3d422-d4dc-4e1b-b1c8-d17566613ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_one[53:-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca0a8f-7aac-4943-861f-fd8f4e2e0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka2_one = vad_fileska2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0703da-de89-43a4-b07d-e15a3a58a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka2_one[54:-8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66452429-1832-46b4-8fc7-6d8ec9109dfb",
   "metadata": {},
   "source": [
    "Try to get the data from 05/24 into a separate array to force it through the vehicle correction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e2742-43f6-4128-a48e-994da0ff3961",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.strptime('20220524', '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36824cb-e679-4a00-a7e5-ef10d89858a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.where(ka1gps.datetime == datetime.strptime('20220524', '%Y%m%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c580e-2a07-4838-b230-68062174e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.where(ka1gps.datetime == np.datetime64(datetime(2022,5,24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab63eec7-a8ab-4d2a-9e54-225118f467fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = np.datetime64(datetime(2022,5,24,))\n",
    "# end_time = np.datetime64(datetime(2022,5,25))\n",
    "# times_0524 = ka1gps[(ka1gps.datetime >= start_time) & (ka1gps.datetime < end_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a225c8-945e-4c01-8cd3-8b8d2eab8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafa889-e9c9-487d-b5e8-c477845c79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1gps_0524_old = ka1gps.loc[19668:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc448a2-5544-43d7-808a-8fb13341c51f",
   "metadata": {},
   "source": [
    "Reset the index numbers so that 19668 becomes 0, hopefully that fixes the function freaking out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b690fc-48a0-4d7d-88f5-3544ed79abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1gps_0524 = ka1gps_0524_old.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812ab22-30d0-4750-a0da-b7a6c8813e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1gps_0524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbacbe2-44cd-4ca8-8256-27f65b7b2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1gps_0524['ddmmyy'] = ka1gps_0524['ddmmyy'].astype(str)\n",
    "ka1gps_0524['hhmmss[UTC]'] = ka1gps_0524['hhmmss[UTC]'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75911e1a-2bcc-4080-8dd4-8a7cc6d34457",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_0524 = ka1gps_0524\n",
    "times_0524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988cacc0-24c6-418b-bced-ec032c2bc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vads_0524_find = dealiased_data.loc[188:]\n",
    "# vads_0524_find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d83576-b606-4ca3-aad7-c5eb7871ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ka1gps_0524_ = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d1815-7b2a-4bb1-8b63-086e6d750939",
   "metadata": {},
   "source": [
    "Use Alex's function to dealias all VADs, including the ones in motion, and download them (radar objects) to netcdf so that I don't have to run this function every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840c0cf-5f5e-4d83-89f7-d9e13e7fb80f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "RUNNING THIS CODE WILL DEALIAS ALL DATA AND WILL TAKE HOURS!!! ALL NETCDFS HAVE ALREADY BEEN SAVED AT THE PATH\n",
    "\n",
    "path = '/Users/juliabman/Desktop/dealiased_data/'\n",
    "de_aliased_vads_ka1 = []\n",
    "for the_file in vad_fileska1:\n",
    "    radar = pyart.io.read(the_file)\n",
    "    print(the_file)\n",
    "    dealiasing = dealias_Ka(radar, PPIGC_flag=False)\n",
    "    de_aliased_vads_ka1.append(dealiasing)\n",
    "    #file_name = the_file.split('/')\n",
    "    new_file_name = pyart.io.write_cfradial(path+the_file[54:-8]+\"_dealiased.nc\", radar)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddf2723-b7ed-4fad-8da7-b48ce2b6fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# RUNNING THIS CODE WILL DEALIAS ALL DATA AND WILL TAKE HOURS!!! ALL NETCDFS HAVE ALREADY BEEN SAVED AT THE PATH\n",
    "\n",
    "path = '/Users/juliabman/Desktop/dealiased_data/ka2/'\n",
    "de_aliased_vads_ka2 = []\n",
    "for the_file in vad_fileska2[200:]: # I have 196 and it errored out so let's see if itll continue\n",
    "    radar = pyart.io.read(the_file)\n",
    "    print(the_file)\n",
    "    dealiasing = dealias_Ka(radar, PPIGC_flag=False)\n",
    "    de_aliased_vads_ka2.append(dealiasing)\n",
    "    #file_name = the_file.split('/')\n",
    "    new_file_name = pyart.io.write_cfradial(path+the_file[54:-8]+\"_dealiased.nc\", radar)\n",
    "    #print(new_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ed413-7f21-461c-a71c-a8f896a5e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(de_aliased_vads_ka2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb94e7b-e5be-4a20-9c3d-9f782fbd06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.size(de_aliased_vads_ka1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb7f87b-b0f0-41d2-ad9d-645876358671",
   "metadata": {},
   "source": [
    "Bring in all the netcdf files for ka1 that can be converted into radar objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3406ad9-57e8-4df3-a882-f8df38ed1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "dealiased_data = sorted(glob.glob('/Users/juliabman/Desktop/dealiased_data/*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e71f68-35d0-4c3a-ad59-a762c82134bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_fileska1 = sorted(glob.glob('/Users/juliabman/Desktop/research2024/product_raw_ka1/*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c07d9-a8e6-4eb8-8a60-37f37aed2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(dealiased_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896cf231-a228-441f-8d33-9ef884019856",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(vad_fileska1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954d938-11c7-4b0d-888a-a2d5a1134e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = vad_fileska1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c1fed-0fe6-4d3e-89e7-423ca1b1fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "one[-18:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b82635-2af4-4a5d-8650-2c926dc50ea4",
   "metadata": {},
   "source": [
    "344 total vad files exist, but only 307 are raw data were produced by the function. The other 37 have already been created by Alex's original vad code and were dealiased netcdfs and not raw data, so the true total is 307."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dfaa1-1877-43f7-8817-7df8c7a23af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_dealiased_data_ka1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836489b4-ed49-47a2-9618-b333193250a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = '/Users/juliabman/Desktop/dealiased_data/'\n",
    "# all_rest_of_ka1 = []\n",
    "# for thefile in vad_fileska1[:]:\n",
    "#     print(thefile)\n",
    "#     radar = pyart.io.read(thefile)\n",
    "#     print(radar)\n",
    "#     write = pyart.io.write_cfradial(path+thefile[-18:-3]+\"_dealiased.nc\", radar)\n",
    "#     print(write)\n",
    "#     all_rest_of_ka1.append(write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7161c9e7-3ff7-4a72-bdff-5d577c22c221",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dealiased_data_ka1 = sorted(glob.glob('/Users/juliabman/Desktop/dealiased_data/Ka1*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6661d3e7-9755-4b6f-ac2f-ebc110c1d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(all_dealiased_data_ka1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a960905-593a-47cf-86e2-13a5be134fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_check = []\n",
    "                        \n",
    "for the_file in all_dealiased_data_ka1[:10]:\n",
    "    read = pyart.io.read_cfradial(the_file) #converts netcdfs back to radar objects!\n",
    "    print(the_file)\n",
    "    times = read.time['data']\n",
    "    time_check.append(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba69b4d-9747-4e35-9995-7f6c3ef1a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can indeed see that they are being read in as radar objects\n",
    "read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62e6014-f80c-4d27-9d71-b536f0f7fb20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_one = pyart.io.read_cfradial(all_dealiased_data_ka1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8451acea-d903-4fa3-95ab-ba9ab4daf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_one.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392c7089-0f43-48ff-ab29-905bd7d5511d",
   "metadata": {},
   "source": [
    "The time field for these vad netcdfs are in seconds since scan started, which is not very useful. It's better to just pull the time from the title of the VAD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c856b4b-56ed-409a-96d6-d71d4e2fc89e",
   "metadata": {},
   "source": [
    "Correct the vads for vehicle motion (can only do one day at a time??):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf2891-f128-4914-91c2-64be50dd3dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corrected_vehicle_vads_0523 = []\n",
    "corrected_vads_file_names_0523 = []\n",
    "for uncorrected_vad in all_dealiased_data_ka1[:]:\n",
    "    print(uncorrected_vad)\n",
    "    read = pyart.io.read_cfradial(uncorrected_vad)\n",
    "    correction_function = vehicle_correction_vad(read, ka1gps)\n",
    "    corrected_vehicle_vads_0523.append(correction_function)\n",
    "    corrected_vads_file_names_0523.append(uncorrected_vad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d0926b-b47f-42d5-b864-843b228f1e9e",
   "metadata": {},
   "source": [
    "The function above (vehicle_correction_vad) for some reason only creates vads from 05/23 and not any from 05/24..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2231e444-d8c6-41bd-9925-a564b25e09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(corrected_vads_file_names_0523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce82fb-b0ba-4ef7-b966-08470d948505",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(corrected_vehicle_vads_0523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ec7f5-4a2a-4a2c-8dc4-a10f6398aa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "1692/9 # 9 categories per list for each radar object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528c282-322a-4076-93c2-609e03260193",
   "metadata": {},
   "source": [
    "Make an array of times for 05/24 so we can force the function to run through the second day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fea040-0ffb-4952-8658-9cc0bd2b7a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dealiased_data_ka1_0524 = all_dealiased_data_ka1[188:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba62da4-bd19-4649-8f68-14d97a562520",
   "metadata": {},
   "outputs": [],
   "source": [
    "dealiased_data_ka1_0524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f5bfa-df54-40a6-95ea-ec2780d261fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(dealiased_data_ka1_0524)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd25edc-70b7-4410-bfa7-c3c3425cc0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1gps[19668:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d480580-a5ff-48ad-97ff-41b53f96101a",
   "metadata": {},
   "source": [
    "Attempting to get it to work through forcing it the dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d1c87-e253-4fcd-b5d7-1239604ad4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_vehicle_vads_0524 = []\n",
    "corrected_vads_file_names_0524 = []\n",
    "for uncorrected_vad in all_dealiased_data_ka1[188:]:\n",
    "    print(uncorrected_vad)\n",
    "    read = pyart.io.read_cfradial(uncorrected_vad)\n",
    "    correction_function = vehicle_correction_vad(read, ka1gps_0524)\n",
    "    corrected_vehicle_vads_0524.append(correction_function)\n",
    "    corrected_vads_file_names_0524.append(uncorrected_vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299c73b-ee82-4183-86f5-2a76594c3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16788796-6077-4be6-adc7-2a6fdb48cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_0524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146f677c-4c3a-421e-bfd1-25d15d37e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vad = corrected_vehicle_vads_0523[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a99734-ec33-4357-a8a1-63ca0033e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012f1bf-dce1-48c2-8c1f-84f2dd154cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vad_radar = first_vad[0] # this shows the corrected_vehicle_vads are lists of objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdfef84-6144-41e5-a355-48f9f7ba4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vad_radar.time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67f8a8-e6f1-4ef6-88b7-3db6c7f6e5a8",
   "metadata": {},
   "source": [
    "Find where in the list the lat and lon are kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b997707-aeb9-4b72-92b7-c060c7d39f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vad_latitude = first_vad[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8747507e-c845-4299-afab-1b3754d3ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vad_latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80899197-ac4b-48f2-a87b-3f2602fb3009",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vad_longitude = first_vad[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ff109-2586-4123-ab4e-0694747fbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_vad_longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d62d89-1687-46bb-9ad9-6c2488673d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_corrected_vad_lats = []\n",
    "ka1_corrected_vad_lons = []\n",
    "for file in corrected_vehicle_vads:\n",
    "    lat = file[5]\n",
    "    lon = file[7]\n",
    "    ka1_corrected_vad_lats.append(lat)\n",
    "    ka1_corrected_vad_lons.append(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cc08b-e5b4-4d09-a666-ea44e3db2114",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(ka1_corrected_vad_lats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a4e9d-782b-4fac-8ed8-8a4c2dfa203c",
   "metadata": {},
   "source": [
    "Grab the times from the vad netcdf file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd5c637-ccf7-4be9-810f-c38ee00bd206",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_ka1_times = []\n",
    "for time_grab in range(len(dealiased_data_ka1)):\n",
    "    file = dealiased_data_ka1[time_grab]\n",
    "    time_yoink = file[43:-13]\n",
    "    time_yoink_datetime = datetime.strptime(time_yoink, '%y%m%d%H%M%S')\n",
    "    vad_ka1_times.append(time_yoink_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220d928-5342-4c52-a89b-2c3ba13bcc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(vad_ka1_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765e860-e6f0-45de-a6e1-eb35b2ea2ca6",
   "metadata": {},
   "source": [
    "Grab the times from the CORRECTED dealiased vad files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558841b9-47aa-4fc3-b49a-761d5b79906d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_ka1_times_corrected_0523 = []\n",
    "for time_grab in range(len(corrected_vads_file_names)):\n",
    "    file = corrected_vads_file_names[time_grab]\n",
    "    time_yoink = file[43:-13]\n",
    "    time_yoink_datetime = datetime.strptime(time_yoink, '%y%m%d%H%M%S')\n",
    "    vad_ka1_times_corrected_0523.append(time_yoink_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b982e44-11d4-40f7-bcea-c1c6006ba23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.size(vad_ka1_times_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be903c86-1256-447d-b5c1-db706449fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_ka1_times_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ef2a7-7101-4b78-bcdc-04a916c970cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_ka1_times_corrected_0524 = []\n",
    "for time_grab in range(len(corrected_vads_file_names)):\n",
    "    file = corrected_vads_file_names[time_grab]\n",
    "    time_yoink = file[43:-13]\n",
    "    time_yoink_datetime = datetime.strptime(time_yoink, '%y%m%d%H%M%S')\n",
    "    vad_ka1_times_corrected_0523.append(time_yoink_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6da8d9-3d16-47b4-aa8d-e6bb6c5d48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobac_file = '/Users/juliabman/Desktop/research2024/tobac_Save/Track.nc'\n",
    "tobac_features_xr = xr.open_dataset(tobac_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73b317-3ed3-41c9-8df7-0d493f0865be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobac_features_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d709e8-40e0-425c-bfc8-b5cd97ec9790",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobac_times = tobac_features_xr['timestr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6157381-1153-4fac-8345-55d60ce2070d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
