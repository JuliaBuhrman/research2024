{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a92d1b-c282-4132-aafe-10f9eb9efd17",
   "metadata": {},
   "source": [
    "# This notebook will be used to plot VADs relative to the storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741374b8-89b2-4d35-b038-7c962c237135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyart\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "# from radarcalc import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import metpy.calc as mpcalc\n",
    "import metpy\n",
    "import metpy.plots\n",
    "from metpy.units import units\n",
    "import cartopy.crs as ccrs\n",
    "import gc\n",
    "from astropy.convolution import convolve\n",
    "from boto.s3.connection import S3Connection\n",
    "import tempfile\n",
    "import copy\n",
    "import matplotlib\n",
    "import xarray as xr\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8fc19-1bf2-4ca7-9e4c-01b1bee6502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    '''\n",
    "    Function to find index of the array in which the value is closest to\n",
    "\n",
    "    Parameters: array (array), value (number)\n",
    "    Returns: index (int)\n",
    "\n",
    "    Example: xind = CM1calc.find_nearest(x,5)\n",
    "    '''\n",
    "\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def dealias_Ka(radar,PPIGC_flag=False):\n",
    "    '''\n",
    "    This function aims to take care of all the nitty gritty customizations of pyarts dealiasing specifically for TTUKa radar data.\n",
    "\n",
    "    Parameter: radar (pyart object), PPIGC_flag (boolean *kwarg)\n",
    "    Returns: radar (pyart object) with corrected_velocity field and velocity_texture field added on\n",
    "\n",
    "    How it works:\n",
    "    1. Calculates velocity texture and creates a histogram based on all of the magnitudes of textures at each bin for the entire volume\n",
    "    2. loops through each indivual sweep\n",
    "    3. infinite loop to iterate between the minimum amount of texture between textures 1 and 6 and the maximum amount of texture in the histogram, with 0.5 m/s steps\n",
    "    4. Each iteration, create a gatefilter that filters the bins above that texture value and dealias it with the gatefilter to ignore the high textured regions\n",
    "    5. If the maximum texture is reached before breaking, set the gatefilter to mask textures above 12 m/s\n",
    "    6. if the scan is an RHI, run a 2 pass variance filter along each ray.\n",
    "        This convolves a 71 sized boxcar with the data, and if the difference between the point the boxcar is centered on and the mean of the boxcar is greater than the nyquist, then add/subtract 2*nyquist to that point\n",
    "    7. Then, take the difference between the mean of the bottom 4 rays of the dealiased velocity in the RHI and the bottom 4 rays of the aliased velocity\n",
    "    8. If the absolute difference is larger than the nyquist, the either subtract 2* nyquist or add 2*nyquist to the entire sweep and break the loop\n",
    "    9. If the absolute difference is less than the nyquist, no fixes need to be applied, break the loop\n",
    "    10. If the scan is a PPI, do steps 8 and 9, but skip steps 6 and 7\n",
    "    11. Outside the infinite loop, add the option of the PPIGC_flag, a boolean flag that will help maintain ground clutter in PPIs at 0 m/s\n",
    "        This works by identifying regions of very low spectrum width (<0.1 m/s), and setting the velocity in those regions = 0. Please note, this may introduce artificial speckles of 0 in real data, where spectrum width is noisy\n",
    "    12. Apply the alias fix algorithm which convolves a boxcar of specified boxcar of size 9 with the data, and if the variance between the middle pixel of interest and the mean is greater than the nyquist, then flip it back over\n",
    "    12. At the end of each sweep, assign the data from that processed sweep into a dictionary, then add the dictionary as the corrected_velocity field.\n",
    "    \n",
    "    The aforementioned method is FAR from perfect, but is as robust as I can do currently. One thing to improve this though is to use the UNRAVEL algorithm: https://github.com/vlouf/dealias\n",
    "    The UNRAVEL algorithm shows remarkable error characteristics compared to \"competitors\", possibly at a time cost, which isn't a HUGE deal for us. Downside is it may not work for our \"volumes\" since they are temporally uncorrelated and are not full volumes through the atmosphere\n",
    "    '''\n",
    "    \n",
    "    vel_texture = pyart.retrieve.calculate_velocity_texture(radar, vel_field='velocity', wind_size=3)\n",
    "    radar.add_field('velocity_texture', vel_texture, replace_existing=True)\n",
    "    hist, bins = np.histogram(radar.fields['velocity_texture']['data'][~np.isnan(radar.fields['velocity_texture']['data'])], bins=150)\n",
    "    bins = (bins[1:]+bins[:-1])/2.0\n",
    "    gatefilter = pyart.filters.GateFilter(radar)\n",
    "    velocity_dealiased = pyart.correct.dealias_region_based(radar, vel_field='velocity', nyquist_vel=radar.instrument_parameters['nyquist_velocity']['data'][0], centered=True) #standin, data will be replaced\n",
    "\n",
    "    for swp_id in range(radar.nsweeps):\n",
    "        #get indices from beginning and ending of sweep\n",
    "        sw_start = radar.sweep_start_ray_index['data'][swp_id]\n",
    "        sw_end = radar.sweep_end_ray_index['data'][swp_id]+1\n",
    "\n",
    "        counter = 0\n",
    "        while True: #do an infinite loop and either break it when the data is unfolded correctly or when the max texture is reached\n",
    "            #if the bin with the lowest count between textures 1 and 6 + i*0.5 is less than the maximum amount of bins\n",
    "            if bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]]+counter*0.5 < np.amax(bins):\n",
    "                gatefilter.exclude_above('velocity_texture', bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]]+counter*0.5)\n",
    "                nyq = radar.instrument_parameters['nyquist_velocity']['data'][0]\n",
    "                vede = pyart.correct.dealias_region_based(radar, vel_field='velocity', nyquist_vel=nyq,\n",
    "                                                                        centered=True, gatefilter=gatefilter)\n",
    "            else:\n",
    "                gatefilter.exclude_above('velocity_texture', 12)#bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]])\n",
    "                nyq = radar.instrument_parameters['nyquist_velocity']['data'][0]\n",
    "                vede = pyart.correct.dealias_region_based(radar, vel_field='velocity', nyquist_vel=nyq,\n",
    "                                                                        centered=True, gatefilter=gatefilter)\n",
    "\n",
    "\n",
    "            np.ma.set_fill_value(vede['data'], np.nan)\n",
    "            #extract mask so we can apply the correct gatefilters on later\n",
    "            mask=np.ma.getmask(vede['data'])\n",
    "\n",
    "            #apply mask to velocity field and fix the small blips from dealiasing\n",
    "            if radar.scan_type == 'rhi':\n",
    "                #pass 1 of variance filtering along the ray.\n",
    "                #Convolves a 71 sized boxcar with the data, and if the difference between the point the boxcar is centered on and the mean of the boxcar is greater than the nyquist, then add/subtract 2*nyquist to that point\n",
    "                vel = vede['data'].filled()\n",
    "\n",
    "                for sw in range(np.shape(vel)[0]):\n",
    "                    mean = convolve(vel[sw,:],np.ones(71))\n",
    "                    var = vel[sw,:]-mean\n",
    "                    high_idx = var > nyq\n",
    "                    low_idx = var < -nyq\n",
    "                    vel[sw,:][high_idx] = vel[sw,:][high_idx] - 2*nyq\n",
    "                    vel[sw,:][low_idx] = vel[sw,:][low_idx] + 2*nyq\n",
    "                vede['data']=np.ma.masked_array(vel,mask=mask,fill_value=np.nan)\n",
    "\n",
    "                #pass 2 of variance filtering along the ray. In case there are errant folds than need to be folded back\n",
    "                vel = vede['data'].filled()\n",
    "                for sw in range(np.shape(vel)[0]):\n",
    "                    mean = convolve(vel[sw,:],np.ones(71))\n",
    "                    var = vel[sw,:]-mean\n",
    "                    high_idx = var > nyq\n",
    "                    low_idx = var < -nyq\n",
    "                    vel[sw,:][high_idx] = vel[sw,:][high_idx] - 2*nyq\n",
    "                    vel[sw,:][low_idx] = vel[sw,:][low_idx] + 2*nyq\n",
    "                vede['data']=np.ma.masked_array(vel,mask=mask,fill_value=np.nan)\n",
    "\n",
    "                #find means of the bottom 4 rays of the RHI(should be close to 0) and compare the dealiased velocities to the aliased velocities\n",
    "                np.ma.set_fill_value(radar.fields['velocity']['data'], np.nan)\n",
    "                meanvelal = np.mean(radar.fields['velocity']['data'][sw_start:sw_start+4,:].filled()[~np.isnan(radar.fields['velocity']['data'][sw_start:sw_start+4,:].filled())])\n",
    "                meanveldeal = np.mean(vede['data'][sw_start:sw_start+4,:].filled()[~np.isnan(vede['data'][sw_start:sw_start+4,:].filled())])\n",
    "                if np.abs(meanvelal-meanveldeal) < nyq: #nyq is an arbitrary threshold and should be tuned\n",
    "                    break\n",
    "                if bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]]+counter*0.5 < np.amax(bins):\n",
    "                    if (meanvelal-meanveldeal) > 0:\n",
    "                        vede['data'][sw_start:sw_end,:] += 2*nyq\n",
    "                    else:\n",
    "                        vede['data'][sw_start:sw_end,:] -= 2*nyq\n",
    "                    break\n",
    "            if radar.scan_type == 'ppi':\n",
    "                np.ma.set_fill_value(radar.fields['velocity']['data'], np.nan)\n",
    "                meanvelal = np.mean(radar.fields['velocity']['data'][sw_start:sw_end,:].filled()[~np.isnan(radar.fields['velocity']['data'][sw_start:sw_end,:].filled())])\n",
    "                meanveldeal = np.mean(vede['data'][sw_start:sw_end,:].filled()[~np.isnan(vede['data'][sw_start:sw_end,:].filled())])\n",
    "                if np.abs(meanvelal-meanveldeal) < nyq: #nyq is an arbitrary threshold and should be tuned\n",
    "                    break\n",
    "                if bins[np.where(hist==np.min(hist[find_nearest(bins,1):find_nearest(bins,6)]))[0][0]]+counter*0.5 < np.amax(bins):\n",
    "                    if (meanvelal-meanveldeal) > 0:\n",
    "                        vede['data'][sw_start:sw_end,:] += 2*nyq\n",
    "                    else:\n",
    "                        vede['data'][sw_start:sw_end,:] -= 2*nyq\n",
    "                    break\n",
    "            counter+=1\n",
    "            \n",
    "        #put alias fix inside here instead of calling it to make it more portable\n",
    "        delta=3\n",
    "        mean = convolve(vede['data'][sw_start:sw_end,:],np.ones((delta,delta))/delta**2.)\n",
    "        mean[0,:] = vede['data'][sw_start:sw_end,:][0,:]\n",
    "        mean[-1,:] = vede['data'][sw_start:sw_end,:][-1,:]\n",
    "        var = vede['data'][sw_start:sw_end,:] - mean\n",
    "\n",
    "        high_idx = np.logical_and(var > nyq, var < 4*nyq)\n",
    "        low_idx = np.logical_and(var < -nyq, var > -4*nyq)\n",
    "\n",
    "        vede['data'][sw_start:sw_end,:][high_idx] = vede['data'][sw_start:sw_end,:][high_idx] - 2*nyq\n",
    "        vede['data'][sw_start:sw_end,:][low_idx] = vede['data'][sw_start:sw_end,:][low_idx] + 2*nyq\n",
    "\n",
    "        #corrects ground clutter by arbitrarily setting the velocity equal to 0 where spectrum width is less than 0.075 m/s\n",
    "        if PPIGC_flag == True:\n",
    "            if radar.scan_type == 'ppi':\n",
    "                sw = radar.fields['spectrum_width']['data'][sw_start:sw_end,:].filled()\n",
    "                vel = radar.fields['velocity']['data'][sw_start:sw_end,:].filled()\n",
    "                mask = sw<0.1\n",
    "                vede['data'][sw_start:sw_end,:] = np.where(~mask,vede['data'][sw_start:sw_end,:],0)\n",
    "\n",
    "        velocity_dealiased['data'][sw_start:sw_end,:] = vede['data'][sw_start:sw_end,:]\n",
    "        velocity_dealiased['data'][sw_start:sw_end,:] = alias_fix(velocity_dealiased['data'][sw_start:sw_end,:],nyq,delta=9)\n",
    "    radar.add_field('corrected_velocity', velocity_dealiased, replace_existing=True)\n",
    "\n",
    "    return radar\n",
    "\n",
    "def alias_fix(vel,nyq,delta=3):\n",
    "    '''\n",
    "    !!!!!!!!!!!!!!!!!!\n",
    "    Removes dealiasing errors around the periphery of a folded region\n",
    "\n",
    "    Parameters: velocity array (array), nyquist velocity (number), size of window (int, must be odd, unity is no change)\n",
    "    Returns: cleaned velocity array (array)\n",
    "    '''\n",
    "    mean = convolve(vel,np.ones((delta,delta))/delta**2.)\n",
    "    mean[0,:] = vel[0,:]\n",
    "    mean[-1,:] = vel[-1,:]\n",
    "    var = vel - mean\n",
    "\n",
    "    high_idx = np.logical_and(var > nyq, var < 4*nyq)\n",
    "    low_idx = np.logical_and(var < -nyq, var > -4*nyq)\n",
    "\n",
    "    vel[high_idx] = vel[high_idx] - 2*nyq\n",
    "    vel[low_idx] = vel[low_idx] + 2*nyq\n",
    "\n",
    "    return vel\n",
    "\n",
    "def get_radar_from_aws_sam(site, datetime_t, datetime_te):\n",
    "    \"\"\"\n",
    "    Get the closest volume of NEXRAD data to a particular datetime.\n",
    "    Parameters\n",
    "    ----------\n",
    "    site : string\n",
    "        four letter radar designation\n",
    "    datetime_t : datetime\n",
    "        desired date time\n",
    "    Returns\n",
    "    -------\n",
    "    radar : Py-ART Radar Object\n",
    "        Radar closest to the queried datetime\n",
    "    \"\"\"\n",
    "    # SAM: added nexradaws to scan bucket instead of boto because boto was causing weird timeout errors...\n",
    "    import nexradaws\n",
    "    import os\n",
    "    \n",
    "    conn = nexradaws.NexradAwsInterface()\n",
    "\n",
    "    scans = conn.get_avail_scans_in_range(datetime_t-timedelta(minutes=15), datetime_te+timedelta(minutes=15), site)\n",
    "\n",
    "    scans = [scan for scan in scans if \"_MDM\" not in scan.filename]\n",
    "\n",
    "    keys = [scan.filename for scan in scans]\n",
    "    datetimes = [scan.scan_time.replace(tzinfo=None) for scan in scans]\n",
    "    \n",
    "    # This is alex's code again:\n",
    "    # find the closest available radar to your datetime\n",
    "    closest_datetime_b = _nearestDate(datetimes, datetime_t)\n",
    "    closest_datetime_e = _nearestDate(datetimes, datetime_te)\n",
    "\n",
    "    index_b = datetimes.index(closest_datetime_b)\n",
    "    index_e = datetimes.index(closest_datetime_e)\n",
    "\n",
    "    radar_namelist = keys[index_b:index_e+1]\n",
    "\n",
    "    # SAM: slightly modified download routine\n",
    "    remote_scan_list = scans[index_b:index_e+1]\n",
    "    radar_list = []\n",
    "    for remote_scan in remote_scan_list:\n",
    "        conn.download(remote_scan, '.')\n",
    "        rdr = pyart.io.read(remote_scan.filename)\n",
    "        radar_list.append(rdr)\n",
    "        os.remove(remote_scan.filename)\n",
    "    return radar_namelist,radar_list\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def get_radar_from_aws(site, datetime_t, datetime_te):\n",
    "    \"\"\"\n",
    "    Get the closest volume of NEXRAD data to a particular datetime.\n",
    "    Parameters\n",
    "    ----------\n",
    "    site : string\n",
    "        four letter radar designation\n",
    "    datetime_t : datetime\n",
    "        desired date time\n",
    "    Returns\n",
    "    -------\n",
    "    radar : Py-ART Radar Object\n",
    "        Radar closest to the queried datetime\n",
    "    \"\"\"\n",
    "\n",
    "    # First create the query string for the bucket knowing\n",
    "    # how NOAA and AWS store the data\n",
    "    my_pref = datetime_t.strftime('%Y/%m/%d/') + site\n",
    "\n",
    "    # Connect to the bucket\n",
    "    conn = S3Connection(anon = True)\n",
    "    bucket = conn.get_bucket('noaa-nexrad-level2')\n",
    "\n",
    "    # Get a list of files\n",
    "    bucket_list = list(bucket.list(prefix = my_pref))\n",
    "\n",
    "    # we are going to create a list of keys and datetimes to allow easy searching\n",
    "    keys = []\n",
    "    datetimes = []\n",
    "\n",
    "    # populate the list\n",
    "    for i in range(len(bucket_list)):\n",
    "        this_str = str(bucket_list[i].key)\n",
    "        if 'gz' in this_str:\n",
    "            endme = this_str[-22:-4]\n",
    "            fmt = '%Y%m%d_%H%M%S_V0'\n",
    "            dt = datetime.strptime(endme, fmt)\n",
    "            datetimes.append(dt)\n",
    "            keys.append(bucket_list[i])\n",
    "\n",
    "        if this_str[-3::] == 'V06':\n",
    "            endme = this_str[-19::]\n",
    "            fmt = '%Y%m%d_%H%M%S_V06'\n",
    "            dt = datetime.strptime(endme, fmt)\n",
    "            datetimes.append(dt)\n",
    "            keys.append(bucket_list[i])\n",
    "\n",
    "    \n",
    "    # find the closest available radar to your datetime\n",
    "    closest_datetime_b = _nearestDate(datetimes, datetime_t)\n",
    "    closest_datetime_e = _nearestDate(datetimes, datetime_te)\n",
    "\n",
    "    index_b = datetimes.index(closest_datetime_b)\n",
    "    index_e = datetimes.index(closest_datetime_e)\n",
    "\n",
    "    radar_namelist = keys[index_b:index_e+1]\n",
    "    radar_list=[]\n",
    "    for i in range(np.shape(radar_namelist)[0]):\n",
    "        localfile = tempfile.NamedTemporaryFile()\n",
    "        radar_namelist[i].get_contents_to_filename(localfile.name)\n",
    "        radar_list.append(pyart.io.read(localfile.name))\n",
    "    return radar_namelist,radar_list\n",
    "\n",
    "def getLocation(lat1, lon1, brng, distancekm):\n",
    "    lat1 = lat1 * np.pi / 180.0\n",
    "    lon1 = lon1 * np.pi / 180.0\n",
    "    #earth radius\n",
    "    R = 6378.1\n",
    "    #R = ~ 3959 MilesR = 3959\n",
    "    bearing = (brng / 90.)* np.pi / 2.\n",
    "\n",
    "    lat2 = np.arcsin(np.sin(lat1) * np.cos(distancekm/R) + np.cos(lat1) * np.sin(distancekm/R) * np.cos(bearing))\n",
    "    lon2 = lon1 + np.arctan2(np.sin(bearing)*np.sin(distancekm/R)*np.cos(lat1),np.cos(distancekm/R)-np.sin(lat1)*np.sin(lat2))\n",
    "    lon2 = 180.0 * lon2 / np.pi\n",
    "    lat2 = 180.0 * lat2 / np.pi\n",
    "    return lat2, lon2\n",
    "\n",
    "def _nearestDate(dates, pivot):\n",
    "    return min(dates, key=lambda x: abs(x - pivot))\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    '''\n",
    "    Function to find index of the array in which the value is closest to\n",
    "\n",
    "    Parameters: array (array), value (number)\n",
    "    Returns: index (int)\n",
    "\n",
    "    Example: xind = CM1calc.find_nearest(x,5)\n",
    "    '''\n",
    "\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array-value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def vehicle_correction_vad(radar,df):\n",
    "    '''\n",
    "    Function that creates a 'vad_corrected_velocity' field that can be used for vad calculations, \n",
    "    but should be general enough to use for stationary VADs as well as moving PPIs. \n",
    "    Other than adding the new field, the radar times are smoothly interpolated and the azimuths are \n",
    "    corrected via the GPS pandas dataframe.\n",
    "    \n",
    "    Parameters: pyart radar object (object), pandas dataframe of appropriate radarGPS file (dataframe)\n",
    "    Returns: pyart radar object (object), speed (float), speed variance (float), bearing (float), bearing variance (float), \n",
    "             latitude (float), latitude variance (float), longitude (float), longitude variance (float)\n",
    "    \n",
    "    Example: radar, velmean, velvar, bearmean, bearvar, latmean, latvar, lonmean, lonvar = vehicle_correction_vad(radar,df)\n",
    "\n",
    "    p.s. only works if the velocity is already dealiased and there is a 'corrected_velocity' field\n",
    "         also only works if a single sweep is extracted, example: radar = radar.extract_sweeps([0])\n",
    "    '''\n",
    "    \n",
    "    #orders the time to increase monotonically instead of having a massive step jump in the middle\n",
    "    roll_mag = (np.argmax(np.abs(np.gradient(radar.time['data'])))+1)\n",
    "    times = np.roll(radar.time['data'],-roll_mag) \n",
    "    \n",
    "    #a complicated way to create linear increasing times (instead of steps) that start at 0 seconds after the time datum and increase to the middle of the second max time plateau (if confused, plotting it is helpful)\n",
    "    #from now on, we are going to assume ray_times is the fractional seconds after the time datum the ray is gathered, and we need to roll it back to match with the rest of the data\n",
    "    ray_times = np.roll(np.arange(0,((np.unique(times)[-2])/(find_nearest(times,np.unique(times)[-2])+int(np.sum(radar.time['data']==np.unique(times)[-2])/2)))*len(times)+1e-11,((np.unique(times)[-2])/(find_nearest(times,np.unique(times)[-2])+int(np.sum(radar.time['data']==np.unique(times)[-2])/2)))),roll_mag)\n",
    "\n",
    "    radar.time['data']=ray_times\n",
    "\n",
    "\n",
    "    df['datetime'] = [datetime.strptime(d,'%d%m%y%H%M%S') for d in df['ddmmyy']+[f'{h:06}' for h in df['hhmmss[UTC]'].astype(int)]]\n",
    "    beginscanindex = df.loc[df['datetime'] == datetime.strptime(radar.time['units'],'seconds since %Y-%m-%dT%H:%M:%SZ')].index\n",
    "    endscanindex = df.loc[df['datetime'] == datetime.strptime(radar.time['units'],'seconds since %Y-%m-%dT%H:%M:%SZ')].index+np.ceil(np.amax(ray_times))+1\n",
    "    \n",
    "\n",
    "    #else:\n",
    "    \n",
    "    dfscan = df.iloc[beginscanindex[0].astype(int):endscanindex[0].astype(int)]\n",
    "    dfscan = dfscan.astype({'Bearing[degrees]': 'float'})\n",
    "    dfscan = dfscan.astype({'Velocity[knots]': 'float'})\n",
    "    \n",
    "    ray_bearings = np.interp(ray_times,np.arange(len(dfscan)),dfscan['Bearing[degrees]'])\n",
    "    ray_speeds = np.interp(ray_times,np.arange(len(dfscan)),dfscan['Velocity[knots]'])\n",
    "    \n",
    "    print('velocity [kts]',dfscan['Velocity[knots]'].mean(),'+-',dfscan['Velocity[knots]'].var())\n",
    "    speed = dfscan['Velocity[knots]'].mean()\n",
    "    print('bearing',dfscan['Bearing[degrees]'].mean(),'+-',dfscan['Bearing[degrees]'].var())\n",
    "    bearing = dfscan['Bearing[degrees]'].mean()\n",
    "    print('latitude',dfscan['Latitude'].astype(float).mean(),'+-',dfscan['Latitude'].astype(float).var())\n",
    "    lat = dfscan['Latitude'].astype(float).mean()\n",
    "    print('longitude',dfscan['Longitude'].astype(float).mean(),'+-',dfscan['Longitude'].astype(float).var())\n",
    "    lon = dfscan['Longitude'].astype(float).mean()\n",
    "    \n",
    "    radar.azimuth['data'] += ray_bearings[:-1] #bearing\n",
    "    \n",
    "    rad_vel = copy.deepcopy(radar.fields['corrected_velocity'])\n",
    "    \n",
    "    rad_vel['data']+=(np.cos(np.deg2rad(radar.azimuth['data']-ray_bearings[:-1]))*(ray_speeds[:-1]/1.94384)*np.cos(np.deg2rad(radar.fixed_angle['data'][0])))[:,np.newaxis]\n",
    "    \n",
    "     #fix mask, remove points very close to radar as well as the very last bin, more often than not, = bad data\n",
    "    rad_vel['data'].mask[:,:5] = True\n",
    "    rad_vel['data'].mask[:,-1] = True\n",
    "    radar.add_field('vad_corrected_velocity', rad_vel, replace_existing=True)\n",
    "    \n",
    "    return radar, dfscan['Velocity[knots]'].mean(),dfscan['Velocity[knots]'].var(),dfscan['Bearing[degrees]'].mean(),dfscan['Bearing[degrees]'].var(),dfscan['Latitude'].astype(float).mean(),dfscan['Latitude'].astype(float).var(),dfscan['Longitude'].astype(float).mean(),dfscan['Longitude'].astype(float).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601b4d8-1f65-4184-a6cc-c7f75d754447",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_vads_df = pd.read_csv('/Users/juliabman/Desktop/vads/vad_dfs/ka1/05232022/ka1_vads_df')\n",
    "ka2_vads_df = pd.read_csv('/Users/juliabman/Desktop/vads/vad_dfs/ka2/05232022/ka2_vads_df')\n",
    "tobac_file = '/Users/juliabman/Desktop/research2024/tobac_Save/Track.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f57e15-123c-4c35-a9dc-81fe9b64ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_vads = sorted(glob.glob('/Users/juliabman/Desktop/vads/vads_radar_objects/ka1/05232022/*.nc'))\n",
    "ka2_vads = sorted(glob.glob('/Users/juliabman/Desktop/vads/vads_radar_objects/ka2/05232022/*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3d931-c5be-4aef-b970-dabdf45ec22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_vads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7ba80-989d-4b60-a63a-34fa611116eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka2_vads_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72feb6e-39c4-4da2-b1f5-fe3df8fe11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ka1_vads_df.drop(columns = 'Unnamed: 0')\n",
    "#ka2_vads_df.drop(columns = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6d8a6-1059-446f-8a0d-dda7f57cf757",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobac_time_array_ka1 = np.array(ka1_vads_df.tobac_time)\n",
    "tobac_time_array_datetime_ka1 = tobac_time_array_ka1.astype('datetime64[s]')\n",
    "\n",
    "tobac_time_array_ka2 = np.array(ka2_vads_df.tobac_time)\n",
    "tobac_time_array_datetime_ka2 = tobac_time_array_ka2.astype('datetime64[s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67a72c-1098-494d-8f6d-18eff83f292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobac_features_xr = xr.open_dataset(tobac_file)\n",
    "idx = tobac_features_xr['idx'].data\n",
    "cell = tobac_features_xr['cell'].data\n",
    "morton_storm_indeces_idx = np.where(idx == 29)\n",
    "morton_storm_indeces = np.where(cell == 29)\n",
    "print(morton_storm_indeces)\n",
    "tobac_times = tobac_features_xr['time']\n",
    "tobac_lats = np.array(tobac_features_xr['latitude'])\n",
    "tobac_lons = np.array(tobac_features_xr['longitude'])\n",
    "\n",
    "#morton_two_prior = morton_storm_indeces[405]\n",
    "#morton_two_post = morton_storm_indeces[839]\n",
    "\n",
    "\n",
    "morton_tobac_lats = tobac_lats[morton_storm_indeces]\n",
    "morton_tobac_lons = tobac_lons[morton_storm_indeces]\n",
    "morton_tobac_times = tobac_times[morton_storm_indeces]\n",
    "morton_cell_idx_29 = cell[morton_storm_indeces]\n",
    "\n",
    "#idx_29_lats = tobac_lats[morton_storm_indeces_idx]\n",
    "#idx_29_lons = tobac_lons[morton_storm_indeces_idx]\n",
    "\n",
    "morton_tobac_times_datetime = morton_tobac_times.astype('datetime64[s]')\n",
    "print(morton_tobac_times_datetime)\n",
    "\n",
    "# print(morton_tobac_lats)\n",
    "# print(morton_tobac_lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e3926-cda3-4853-a5e8-8e15eed78250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "def calc_velocity(lat1,lon1,lat2,lon2,time1,time2):\n",
    "    R = 6371 # Radius of the earth in km\n",
    "    dLat = radians(lat2-lat1)\n",
    "    dLon = radians(lon2-lon1)\n",
    "    rLat1 = radians(lat1)\n",
    "    rLat2 = radians(lat2)\n",
    "    a = sin(dLat/2) * sin(dLat/2) + cos(rLat1) * cos(rLat2) * sin(dLon/2) * sin(dLon/2) \n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    d = R * c * 1000 # Distance in m\n",
    "    return d / (time2 - time1) # dividing by 1e9 because the times are in nanoseconds\n",
    "\n",
    "def get_bearing(lat1, long1, lat2, long2):\n",
    "    dLon = (long2 - long1)\n",
    "    x = cos(radians(lat2)) * sin(radians(dLon))\n",
    "    y = cos(radians(lat1)) * sin(radians(lat2)) - sin(radians(lat1)) * cos(radians(lat2)) * cos(radians(dLon))\n",
    "    brng = np.arctan2(x,y)\n",
    "    brng = (np.degrees(brng)+180) % 360\n",
    "    return brng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bacf44-0e64-4c04-ae79-de0c2f7c6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR KA1\n",
    "storm_velocity=[]\n",
    "storm_direction=[]\n",
    "#tobac_time_array_ka1 = np.array(ka1_vads_df.tobac_time)\n",
    "time_initial = tobac_time_array_datetime_ka1[0].astype(float)\n",
    "time_final = tobac_time_array_datetime_ka1[-1].astype(float)\n",
    "\n",
    "for i in np.arange(0,19): # the range of the first tobac time index that is closest to the ka1 time and the last tobac time index closest to ka1 time\n",
    "    velocity = calc_velocity(morton_tobac_lats[i], morton_tobac_lons[i],\n",
    "                             morton_tobac_lats[i+1],morton_tobac_lons[i+1],\n",
    "                             time_initial, time_final)\n",
    "                             #storm_decimalsec[i],storm_decimalsec[i+1])\n",
    "    storm_velocity.append(velocity)\n",
    "    \n",
    "    direction = get_bearing(morton_tobac_lats[i],morton_tobac_lons[i],\n",
    "                            morton_tobac_lats[i+1], morton_tobac_lons[i+1])\n",
    "    storm_direction.append(direction)\n",
    "    \n",
    "storm_velocity = np.append([np.nan],storm_velocity) # in meters/sec\n",
    "storm_direction = np.append([np.nan],storm_direction) # in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776e5f8-36a7-48bf-befb-8f7a8ca873d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48eab4-76c5-40e2-b6f5-be75aa271165",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_directiondf = pd.DataFrame(storm_direction)\n",
    "\n",
    "direction_corrected = storm_directiondf.dropna()\n",
    "\n",
    "storm_directiondf= pd.DataFrame(storm_direction)\n",
    "\n",
    "direction_corrected = storm_directiondf.dropna()\n",
    "\n",
    "direction_corrected_array = np.array(direction_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5cf295-2115-4365-806e-ff435d646d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.05, 0.2, 0.5, 0.2, 0.05])\n",
    "storm_direction_weighted_ka1 = []\n",
    "\n",
    "for i in range(len((direction_corrected))):\n",
    "    j = i+2 # if we start any further it'll loop to the back values of the array and average them\n",
    "    #print(j)\n",
    "    storm_direction_over_25_minutes = np.array([direction_corrected_array[j-2], direction_corrected_array[j-1], \n",
    "                                            direction_corrected_array[j], direction_corrected_array[j+1], direction_corrected_array[j+2]])\n",
    "    #print(storm_direction_over_25_minutes)\n",
    "    array = np.array(storm_direction_over_25_minutes)\n",
    "    flat = array.flatten()\n",
    "    #print(np.shape(flat))\n",
    "    #print(np.shape(weights))\n",
    "    #print(storm_direction_over_25_minutes)\n",
    "    #storm_avg_weighted = sum(storm_direction_over_25_minutes * weights)/ sum(weights)\n",
    "    storm_avg_weighted_ka1 = np.average(flat, weights = weights)\n",
    "    print(storm_avg_weighted_ka1)\n",
    "    storm_direction_weighted_ka1.append(storm_avg_weighted_ka1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f01c6b-6d0b-4cdc-afa9-3b1a2d456f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_now = ka1_vads_df[ka1_vads_df.tobac_time_indeces < 15]\n",
    "for_now_index = for_now.tobac_time_indeces\n",
    "for_now_array = np.array(for_now_index)\n",
    "\n",
    "storm_direction_for_ka1_list = []\n",
    "for t in for_now_array:\n",
    "    #print(t)\n",
    "    storm_direction_for_ka1 = storm_direction_weighted_ka1[t]\n",
    "    storm_direction_for_ka1_list.append(storm_direction_for_ka1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a8c6e-0cd1-4e67-8226-f7896e22af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR KA2\n",
    "storm_velocity_ka2=[]\n",
    "storm_direction_ka2=[]\n",
    "\n",
    "#tobac_time_array_ka2 = np.array(ka2_vads_df.tobac_time)\n",
    "time_initial_ka2 = tobac_time_array_datetime_ka2[0].astype(float)\n",
    "time_final_ka2 = tobac_time_array_datetime_ka2[-1].astype(float)\n",
    "\n",
    "for i in np.arange(0,38): # the range of the first tobac time index that is closest to the ka1 time and the last tobac time index closest to ka1 time\n",
    "    velocity_ka2 = calc_velocity(morton_tobac_lats[i], morton_tobac_lons[i],\n",
    "                             morton_tobac_lats[i+1],morton_tobac_lons[i+1],\n",
    "                             time_initial_ka2, time_final_ka2)\n",
    "                             #storm_decimalsec[i],storm_decimalsec[i+1])\n",
    "    storm_velocity_ka2.append(velocity_ka2)\n",
    "    \n",
    "    direction_ka2 = get_bearing(morton_tobac_lats[i],morton_tobac_lons[i],\n",
    "                            morton_tobac_lats[i+1], morton_tobac_lons[i+1])\n",
    "    storm_direction_ka2.append(direction_ka2)\n",
    "    \n",
    "storm_velocity_ka2 = np.append([np.nan],storm_velocity_ka2) # in meters/sec\n",
    "storm_direction_ka2 = np.append([np.nan],storm_direction_ka2) # in degrees\n",
    "\n",
    "storm_directiondf_ka2 = pd.DataFrame(storm_direction_ka2)\n",
    "\n",
    "direction_corrected_ka2 = storm_directiondf_ka2.dropna()\n",
    "\n",
    "storm_directiondf_ka2= pd.DataFrame(storm_direction_ka2)\n",
    "\n",
    "direction_corrected_ka2 = storm_directiondf_ka2.dropna()\n",
    "\n",
    "direction_corrected_array_ka2 = np.array(direction_corrected_ka2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b63e6-3650-4bb7-a110-b3c95731b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.05, 0.2, 0.5, 0.2, 0.05])\n",
    "storm_direction_weighted_ka2 = []\n",
    "\n",
    "for i in range(len((direction_corrected_ka2))):\n",
    "    j = i+2 # if we start any further it'll loop to the back values of the array and average them\n",
    "    #print(j)\n",
    "    storm_direction_over_25_minutes_ka2 = np.array([direction_corrected_array_ka2[j-2], direction_corrected_array_ka2[j-1], \n",
    "                                            direction_corrected_array_ka2[j], direction_corrected_array_ka2[j+1], direction_corrected_array_ka2[j+2]])\n",
    "    #print(storm_direction_over_25_minutes)\n",
    "    array_ka2 = np.array(storm_direction_over_25_minutes_ka2)\n",
    "    flat_ka2 = array_ka2.flatten()\n",
    "    #print(np.shape(flat))\n",
    "    #print(np.shape(weights))\n",
    "    #print(storm_direction_over_25_minutes)\n",
    "    #storm_avg_weighted = sum(storm_direction_over_25_minutes * weights)/ sum(weights)\n",
    "    storm_avg_weighted_ka2 = np.average(flat_ka2, weights = weights)\n",
    "    print(storm_avg_weighted_ka2)\n",
    "    storm_direction_weighted_ka2.append(storm_avg_weighted_ka2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93efc3d8-47d9-4858-ae4e-789140e889c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_now_ka2 = ka2_vads_df[ka2_vads_df.tobac_time_indeces < 34]\n",
    "for_now_index_ka2 = for_now_ka2.tobac_time_indeces\n",
    "print(for_now_ka2)\n",
    "for_now_array_ka2 = np.array(for_now_index_ka2)\n",
    "\n",
    "storm_direction_for_ka2_list = []\n",
    "for t2 in for_now_array_ka2:\n",
    "    #print(t)\n",
    "    storm_direction_for_ka2 = storm_direction_weighted_ka2[t2]\n",
    "    storm_direction_for_ka2_list.append(storm_direction_for_ka2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da607393-2566-4008-b312-9504c2b6bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR KA1\n",
    "bearing_storm_ka1=[]\n",
    "tobac_time_array_ka1 = np.array(ka1_vads_df.tobac_time)\n",
    "\n",
    "for i in range(len(ka1_vads_df.tobac_lats)): # the range of the first tobac time index that is \n",
    "                            #closest to the ka1 time and the last tobac time index closest to ka1 time                            \n",
    "    \n",
    "    direction = get_bearing(ka1_vads_df.tobac_lats[i],ka1_vads_df.tobac_lons[i],\n",
    "                            ka1_vads_df.Latmean[i], ka1_vads_df.Lonmean[i])\n",
    "    bearing_storm_ka1.append(direction)\n",
    "bearing_storm_ka1 = np.append([np.nan],bearing_storm_ka1) # in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c456fef5-5435-47ce-b50f-0c4fcf5184ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearing_df = pd.DataFrame(bearing_storm_ka1)\n",
    "\n",
    "bearing_storm_ka1_corrected_df = bearing_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf72c7-2afc-4c3f-a45c-b65b7900f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearing_storm_ka1_corrected = np.array(bearing_storm_ka1_corrected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110ad12-5ac4-4c32-b76f-7dbb3cf27aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_motion_correction = 90 -np.array(storm_direction_for_ka1_list)\n",
    "\n",
    "bearing_storm_ka1_shifted_list = []\n",
    "for thing in range(len(storm_motion_correction)):\n",
    "    bearing_storm_ka1_shifted = bearing_storm_ka1_corrected[thing] + storm_motion_correction[thing]\n",
    "    bearing_storm_ka1_shifted_list.append(bearing_storm_ka1_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897c3f6-f699-4a14-8de1-b48af40ad222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR KA2\n",
    "bearing_storm_ka2=[]\n",
    "tobac_time_array_ka2 = np.array(ka2_vads_df.tobac_time)\n",
    "\n",
    "for i_2 in range(len(ka2_vads_df.tobac_lats)): # the range of the first tobac time index that is \n",
    "                            #closest to the ka1 time and the last tobac time index closest to ka1 time                            \n",
    "    \n",
    "    direction2 = get_bearing(ka2_vads_df.tobac_lats[i_2],ka2_vads_df.tobac_lons[i_2],\n",
    "                            ka2_vads_df.Latmean[i_2], ka2_vads_df.Lonmean[i_2])\n",
    "    bearing_storm_ka2.append(direction2)\n",
    "bearing_storm_ka2 = np.append([np.nan],bearing_storm_ka2) # in degrees\n",
    "\n",
    "\n",
    "bearing_df_2 = pd.DataFrame(bearing_storm_ka2)\n",
    "bearing_storm_ka2_corrected_df = bearing_df_2.dropna()\n",
    "bearing_storm_ka2_corrected = np.array(bearing_storm_ka2_corrected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf803df-638f-4010-a27f-3ae6e5b0104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "storm_motion_correction_ka2 = 90 - np.array(storm_direction_for_ka2_list)\n",
    "\n",
    "bearing_storm_ka2_shifted_list = []\n",
    "for thing_2 in range(len(storm_motion_correction_ka2)):\n",
    "    bearing_storm_ka2_shifted = bearing_storm_ka2_corrected[thing_2] + storm_motion_correction_ka2[thing_2]\n",
    "    bearing_storm_ka2_shifted_list.append(bearing_storm_ka2_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5d5f43-2a74-481d-bb29-4c467e739a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in the figure\n",
    "distance_ka1_from_storm = ka1_vads_df.Distance_From_Storm\n",
    "distance_ka2_from_storm = ka2_vads_df.Distance_From_Storm\n",
    "\n",
    "# bring in the figure\n",
    "\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.add_subplot(projection = 'polar')\n",
    "ax.set_rmax(2)\n",
    "ax.set_rlabel_position(-22.5)  # Move radial labels away from plotted line\n",
    "ax.grid(True, linestyle = ':') # makes grid lines dotted\n",
    "ax.set_title(\"Ka1 and Ka2 Scan Locations Adjusted by Storm Motion Using tobac\")\n",
    "ax.plot()\n",
    "\n",
    "# now that we have the figure set up, we can plot on it\n",
    "# r --> great circle distance from lat and lon of storm to lat and lon of vehicle\n",
    "# theta --> corrected bearing\n",
    "# in polar plots the order of plotting is (theta, r)\n",
    "\n",
    "theta = np.array(bearing_storm_ka1_shifted_list)[22:] -90\n",
    "theta_radians = theta * (np.pi/180)\n",
    "r = distance_ka1_from_storm[22:120]# - np.array(ka1_matching_velocity) - np.array(storm_velocity)\n",
    "\n",
    "#print(theta_radians)\n",
    "#print(r)\n",
    "print(np.shape(theta))\n",
    "print(np.shape(r))\n",
    "\n",
    "theta2 = np.array(bearing_storm_ka2_shifted_list) -90\n",
    "theta2_radians = theta2 * (np.pi/180)\n",
    "r2 = distance_ka2_from_storm[0:121]\n",
    "\n",
    "print(np.shape(theta2_radians))\n",
    "print(np.shape(r2))\n",
    "\n",
    "ka1 = ax.scatter(theta_radians, r, s = 10) # s changes point size\n",
    "ka2 = ax.scatter(theta2_radians, r2, s = 10)\n",
    "\n",
    "ax.legend(['ka1', 'ka2'])\n",
    "# plt.savefig('/Users/juliabman/Desktop/Seminar_Pictures/both_ka_positions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a8066-4291-4261-b473-7646502729b7",
   "metadata": {},
   "source": [
    "Need to get the radar objects that align with the plotted points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c9fba-2fb9-4bca-ad12-d496de541aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_vads_read = []\n",
    "for vad in ka1_vads:\n",
    "    read = pyart.io.read(vad)\n",
    "    ka1_vads_read.append(read)\n",
    "\n",
    "ka2_vads_read = []\n",
    "for vad in ka2_vads:\n",
    "    read2 = pyart.io.read(vad)\n",
    "    ka2_vads_read.append(read2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c80ee-dcd4-45c5-9bcb-1fd0f717242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_vads_df.Distance_From_Storm.values[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d6aed-e792-4156-9656-182760edcb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97081751-4b80-4e37-8efe-f952bf8b2d8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3372e2-a62f-4455-8363-8260cf540a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_r = pd.Series(r)\n",
    "ka1_theta=pd.Series(theta_radians.flatten())\n",
    "ka1_radar = pd.Series(ka1_vads_read)\n",
    "ka1_vads_filenames = pd.Series(ka1_vads)\n",
    "ka1_df=pd.DataFrame(pd.concat([ka1_r, ka1_theta, ka1_radar, ka1_vads_filenames],axis=1))\n",
    "ka1_df.columns=['r','theta','radar','filename']\n",
    "print(np.shape(ka1_df))\n",
    "\n",
    "#ka1_in_inflow=ka1_df[(ka1_df.r <= 40) & (ka1_df.theta <=90) or (ka1_df.theta > 270)]\n",
    "\n",
    "ka1_negative_angles=ka1_df[(ka1_df.theta < 0)]\n",
    "ka1_positive_angles=ka1_df[(ka1_df.theta >= 0)]\n",
    "#print(ka1_negative_angles)\n",
    "\n",
    "ka1_in_inflow_negative=ka1_negative_angles[(ka1_negative_angles.theta >= -(np.pi/2)) | (ka1_negative_angles.theta <=-(3 * np.pi)/2) & (ka1_negative_angles.r <=40)]\n",
    "#ka1_in_inflow_positive = ka1_positive_angles[(ka1_positive_angles.theta <= (np.pi/2)) | (ka1_positive_angles.theta >= (3 * np.pi)/2) & (ka1_positive_angles.r <= 40)]\n",
    "\n",
    "print(np.shape(ka1_in_inflow_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1068b-754c-4361-8c45-098feb1eea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_in_inflow_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a30795b-5439-4c45-93c5-ee230de5b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ka1_r = pd.Series(r)\n",
    "# ka1_theta=pd.Series(theta_radians.flatten())\n",
    "\n",
    "# r_indeces = ka1_r.index.values\n",
    "# ka1_radar = pd.Series(ka1_vads_read)\n",
    "# ka1_vads_in_inflow = ka1_radar[r_indeces]\n",
    "# ka1_vads_in_inflow_pd = pd.Series(ka1_vads_in_inflow)\n",
    "\n",
    "# ka1_df=pd.DataFrame(pd.concat([ka1_r, ka1_theta, ka1_vads_in_inflow_pd],axis=1))\n",
    "# ka1_df.columns=['r','theta','radar']\n",
    "# print(np.shape(ka1_df))\n",
    "\n",
    "# #ka1_in_inflow=ka1_df[(ka1_df.r <= 40) & (ka1_df.theta <=90) or (ka1_df.theta > 270)]\n",
    "\n",
    "# ka1_negative_angles=ka1_df[(ka1_df.theta < 0)]\n",
    "# ka1_positive_angles=ka1_df[(ka1_df.theta >= 0)]\n",
    "# #print(ka1_negative_angles)\n",
    "\n",
    "# ka1_in_inflow_negative=ka1_negative_angles[(ka1_negative_angles.theta >= -(np.pi/2)) | (ka1_negative_angles.theta <=-(3 * np.pi)/2) & (ka1_negative_angles.r <=40)]\n",
    "# #ka1_in_inflow_positive = ka1_positive_angles[(ka1_positive_angles.theta <= (np.pi/2)) | (ka1_positive_angles.theta >= (3 * np.pi)/2) & (ka1_positive_angles.r <= 40)]\n",
    "\n",
    "# print(np.shape(ka1_in_inflow_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa84c6e-3aea-4e47-ad5b-8563f18e2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_in_inflow_negative = ka1_in_inflow_negative.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403773ed-b4e5-4378-8ed3-b81e5a2b1d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_in_inflow_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119661f-3b45-4de5-9562-bc1b5839c52c",
   "metadata": {},
   "source": [
    "Check that the times of the vads align with when they were in the inflow or even plot the vads with the reflectivity thing alex made to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b47cf0-36bb-4d08-b668-bac3dbec6e5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in ka1_in_inflow_negative.radar:\n",
    "    fig = plt.figure()\n",
    "    radar_map_display_r = pyart.graph.RadarMapDisplay(t)\n",
    "    radar_map_display_r.plot_ppi_map('corrected_velocity_terminal', cmap = 'pyart_balance', vmin = -40, vmax = 40)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689870b8-d73b-4d19-acd3-6e5296dc7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka2_r = pd.Series(r2)\n",
    "ka2_theta=pd.Series(theta2_radians.flatten())\n",
    "ka2_radar = pd.Series(ka2_vads_df.Radar)\n",
    "ka2_df=pd.DataFrame(pd.concat([ka2_r,ka2_theta, ka2_radar],axis=1))\n",
    "ka2_df.columns=['r2','theta2','radar2']\n",
    "print(np.shape(ka2_df))\n",
    "\n",
    "#ka1_in_inflow=ka1_df[(ka1_df.r <= 40) & (ka1_df.theta <=90) or (ka1_df.theta > 270)]\n",
    "\n",
    "ka2_negative_angles=ka2_df[(ka2_df.theta2 < 0)]\n",
    "ka2_positive_angles=ka2_df[(ka2_df.theta2 >= 0)]\n",
    "#print(ka1_negative_angles)\n",
    "\n",
    "ka2_in_inflow_negative=ka2_negative_angles[(ka2_negative_angles.theta2 >= -(np.pi/2)) | (ka2_negative_angles.theta2 <=-(3 * np.pi)/2) & (ka2_negative_angles.r2 <=40)]\n",
    "#ka1_in_inflow_positive = ka1_positive_angles[(ka1_positive_angles.theta <= (np.pi/2)) | (ka1_positive_angles.theta >= (3 * np.pi)/2) & (ka1_positive_angles.r <= 40)]\n",
    "\n",
    "print(np.shape(ka2_in_inflow_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153dc62-f793-4382-8b24-9e11c95c102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ka1_vads_df[42:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c951128-cca1-4c81-befc-51f8250c88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in the figure\n",
    "distance_ka1_from_storm = ka1_vads_df.Distance_From_Storm\n",
    "distance_ka2_from_storm = ka2_vads_df.Distance_From_Storm\n",
    "\n",
    "# bring in the figure\n",
    "\n",
    "fig = plt.figure(figsize = (5,5))\n",
    "ax = fig.add_subplot(projection = 'polar')\n",
    "ax.set_rmax(2)\n",
    "ax.set_rlabel_position(-22.5)  # Move radial labels away from plotted line\n",
    "ax.grid(True, linestyle = ':') # makes grid lines dotted\n",
    "ax.set_title(\"Ka1 and Ka2 Scan Locations Adjusted by Storm Motion Using tobac\")\n",
    "ax.plot()\n",
    "\n",
    "# now that we have the figure set up, we can plot on it\n",
    "# r --> great circle distance from lat and lon of storm to lat and lon of vehicle\n",
    "# theta --> corrected bearing\n",
    "# in polar plots the order of plotting is (theta, r)\n",
    "\n",
    "# theta = np.array(bearing_storm_ka1_shifted_list)[22:] -90\n",
    "# theta_radians = theta * (np.pi/180)\n",
    "# r = distance_ka1_from_storm[22:120]# - np.array(ka1_matching_velocity) - np.array(storm_velocity)\n",
    "\n",
    "# #print(theta_radians)\n",
    "# #print(r)\n",
    "# print(np.shape(theta))\n",
    "# print(np.shape(r))\n",
    "\n",
    "# theta2 = np.array(bearing_storm_ka2_shifted_list) -90\n",
    "# theta2_radians = theta2 * (np.pi/180)\n",
    "# r2 = distance_ka2_from_storm[0:121]\n",
    "\n",
    "# print(np.shape(theta2_radians))\n",
    "# print(np.shape(r2))\n",
    "\n",
    "ka1 = ax.scatter(ka1_in_inflow_negative.theta, ka1_in_inflow_negative.r, s = 10) # s changes point size\n",
    "ka2 = ax.scatter(ka2_in_inflow_negative.theta2, ka2_in_inflow_negative.r2, s = 10)\n",
    "\n",
    "ax.legend(['ka1', 'ka2'])\n",
    "# plt.savefig('/Users/juliabman/Desktop/Seminar_Pictures/both_ka_positions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76eafef-b3ef-4ede-8206-0d2afb6cd653",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for vad in ka1_vads[50:55]:\n",
    "    read = pyart.io.read(vad)\n",
    "    fig = plt.figure()\n",
    "    rmd = pyart.graph.RadarMapDisplay(read)\n",
    "    rmd.plot_ppi_map('corrected_velocity_terminal', cmap = 'pyart_balance', vmin = -40, vmax = 40)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b116bf3-4441-4e63-a84b-b0ac421fc5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 30\n",
    "MEDIUM_SIZE = 40\n",
    "BIGGER_SIZE = 60\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "#vad_files = test\n",
    "#vadu_list = []\n",
    "#vadv_list = []\n",
    "\n",
    "#119 east\n",
    "#170 south\n",
    "#186 west\n",
    "#191 north\n",
    "for thefile in ka1_in_inflow_negative.filename:\n",
    "    print(thefile)\n",
    "    radar = pyart.io.read(thefile)\n",
    "    if np.logical_and(radar.scan_type == 'ppi', radar.fixed_angle['data'][0] > 10):\n",
    "        \n",
    "        radar = radar.extract_sweeps([0])\n",
    "        #radar = dealias_Ka(radar)\n",
    "        \n",
    "        df = pd.read_csv('/Users/juliabman/Desktop/research2024/GPS_Ka1_20220523.txt', dtype=str)\n",
    "        \n",
    "        radar, velmean, velvar, bearmean, bearvar, latmean, latvar, lonmean, lonvar = vehicle_correction_vad(radar,df)\n",
    "        radar = pyart.io.read(thefile)\n",
    "\n",
    "        try:\n",
    "            VAD = pyart.retrieve.vad_browning(radar, 'corrected_velocity_terminal', z_want=np.arange(50,5001,10),gatefilter=None)\n",
    "        except:\n",
    "            print('not enough data')\n",
    "            continue\n",
    "        vadu = VAD.u_wind*1.94384 # meters to knots\n",
    "        vadv = VAD.v_wind*1.94384\n",
    "        print(f'u velocity {vadu[:10]}')\n",
    "        print(f'v velocity {vadv[:10]}')\n",
    "        \n",
    "        fig = plt.figure(figsize=(51, 17),facecolor='white')\n",
    "        display = pyart.graph.RadarDisplay(radar)\n",
    "        \n",
    "        ax1 = plt.subplot2grid((1,3),(0,0))\n",
    "        display.plot_ppi('corrected_velocity_terminal', 0, cmap='pyart_balance', ax=ax1, vmin=-40., vmax=40.,\n",
    "                             colorbar_label=r'Radial Velocity [m s$^{-1}$]') #title='Motion Corrected Radial Velocity')\n",
    "        \n",
    "        ax3 = plt.subplot2grid((1,3),(0,2))\n",
    "        \n",
    "        hodo = metpy.plots.Hodograph(ax3,component_range=40.)\n",
    "        hodo.add_grid(increment=5)\n",
    "        \n",
    "        hodo.plot(vadu,vadv)\n",
    "\n",
    "        ax3.set_xlabel('U-component Wind [knots]')\n",
    "        ax3.set_ylabel('V-component Wind [knots]')\n",
    "        ax3.set_title('Vehicle motion corrected VAD')\n",
    "\n",
    "        # base = pyplot.gca().transData\n",
    "        # rot = transforms.Affine2D().rotate_deg(90)\n",
    "        \n",
    "        for i,z in enumerate([0,95,195,295,395,495]):\n",
    "            try:\n",
    "                if np.isnan(vadu[z]):\n",
    "                    pass\n",
    "                else:\n",
    "                    circle = plt.Circle((vadu[z], vadv[z]), radius=1.5,color='k',zorder=30)\n",
    "                    ax3.add_patch(circle)\n",
    "                    label = ax3.annotate(i, xy=(vadu[z], vadv[z]), fontsize=12, ha=\"center\",va='center',color='white',zorder=30,weight=\"bold\")\n",
    "            except: pass\n",
    "        \n",
    "        \n",
    "        r=35\n",
    "        radar_namelist, radar_list = get_radar_from_aws_sam('KLBB', datetime.strptime(thefile.split('/')[-1].split('.')[0][29:],'%Y%m%d%H%M%S'), datetime.strptime(thefile.split('/')[-1].split('.')[0][29:],'%Y%m%d%H%M%S')+timedelta(minutes=5))\n",
    "        print(radar_namelist)\n",
    "        print(radar_list)\n",
    "        #scantime88d = datetime.strptime(str(radar_namelist)[40:57], '%Y%m%d_%H%M%S') #SAM DELETED THAT\n",
    "        swp_id = 0\n",
    "        radar88d = radar_list[0].extract_sweeps([0])\n",
    "        scantime88d = pyart.util.datetime_from_radar(radar88d)\n",
    "        # SAM ADDED THAT^^^\n",
    "        time_text = ' ' + str(radar88d.time['units'][-9:1])\n",
    "        title ='KLBB PPI ' +scantime88d.strftime(\"%m/%d/%y %H:%M\")+ ' UTC'\n",
    "        display = pyart.graph.RadarMapDisplay(radar88d)\n",
    "        ax2 = plt.subplot2grid((1,3),(0,1),projection=display.grid_projection)\n",
    "        display.plot_ppi_map('reflectivity', swp_id, cmap='pyart_HomeyerRainbow', ax=ax2, vmin=0., vmax=75., \n",
    "                             colorbar_label='Radar Reflectivity [dBZ]', title='Reflectivity',\n",
    "                             min_lon=getLocation(latmean, lonmean, 270, r)[1],\n",
    "                             max_lon=getLocation(latmean, lonmean, 90, r)[1],\n",
    "                             min_lat=getLocation(latmean, lonmean, 180, r)[0],\n",
    "                             max_lat=getLocation(latmean, lonmean, 0, r)[0])\n",
    "        \n",
    "        m = matplotlib.markers.MarkerStyle(marker='^')\n",
    "        m._transform = m.get_transform().rotate_deg(-bearmean)\n",
    "        ax2.plot(lonmean,latmean,color='k',marker=m,ms=20,transform=ccrs.PlateCarree())\n",
    "        m = matplotlib.markers.MarkerStyle(marker=3)\n",
    "        m._transform = m.get_transform().rotate_deg(-bearmean)\n",
    "        ax2.plot(lonmean,latmean,color='k',marker=m,ms=50,transform=ccrs.PlateCarree(),markeredgewidth=4)\n",
    "\n",
    "\n",
    "        fname=thefile.split('/')[-1].split('.')[0][:]\n",
    "        plt.suptitle(fname)#+' '+str(np.around(dfscan['Bearing[degrees]'].astype(float).var(),decimals=2))+' '+str(np.around(dfscan['Velocity[knots]'].astype(float).var(),decimals=2)))\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig(f\"/Users/juliabman/Desktop/alex_morton_hodos/{fname}.png\")\n",
    "        # plt.close()\n",
    "        # fig.clf()\n",
    "        # gc.collect()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69ab39-b7f8-4cda-b289-2fbaf957606d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SMALL_SIZE = 30\n",
    "MEDIUM_SIZE = 40\n",
    "BIGGER_SIZE = 60\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "#vad_files = test\n",
    "#vadu_list = []\n",
    "#vadv_list = []\n",
    "\n",
    "#119 east\n",
    "#170 south\n",
    "#186 west\n",
    "#191 north\n",
    "for thefile in ka1_in_inflow_negative.filename:\n",
    "    print(thefile)\n",
    "    radar = pyart.io.read(thefile)\n",
    "    if np.logical_and(radar.scan_type == 'ppi', radar.fixed_angle['data'][0] > 10):\n",
    "        \n",
    "        radar = radar.extract_sweeps([0])\n",
    "        #radar = dealias_Ka(radar)\n",
    "        \n",
    "        df = pd.read_csv('/Users/juliabman/Desktop/research2024/GPS_Ka1_20220523.txt', dtype=str)\n",
    "        \n",
    "        radar, velmean, velvar, bearmean, bearvar, latmean, latvar, lonmean, lonvar = vehicle_correction_vad(radar,df)\n",
    "        radar = pyart.io.read(thefile)\n",
    "\n",
    "        try:\n",
    "            VAD = pyart.retrieve.vad_browning(radar, 'velocity', z_want=np.arange(50,5001,10),gatefilter=None)\n",
    "        except:\n",
    "            print('not enough data')\n",
    "            continue\n",
    "        vadu = VAD.u_wind*1.94384 # meters to knots\n",
    "        vadv = VAD.v_wind*1.94384\n",
    "        print(f'u velocity {vadu[:10]}')\n",
    "        print(f'v velocity {vadv[:10]}')\n",
    "        \n",
    "        fig = plt.figure(figsize=(51, 17),facecolor='white')\n",
    "        display = pyart.graph.RadarDisplay(radar)\n",
    "        \n",
    "        ax1 = plt.subplot2grid((1,3),(0,0))\n",
    "        display.plot_ppi('velocity', 0, cmap='pyart_balance', ax=ax1, vmin=-40., vmax=40.,\n",
    "                             colorbar_label=r'Radial Velocity [m s$^{-1}$]') #title='Motion Corrected Radial Velocity')\n",
    "        \n",
    "        ax3 = plt.subplot2grid((1,3),(0,2))\n",
    "        \n",
    "        hodo = metpy.plots.Hodograph(ax3,component_range=40.)\n",
    "        hodo.add_grid(increment=5)\n",
    "        \n",
    "        hodo.plot(vadu,vadv)\n",
    "\n",
    "        ax3.set_xlabel('U-component Wind [knots]')\n",
    "        ax3.set_ylabel('V-component Wind [knots]')\n",
    "        ax3.set_title('Vehicle motion corrected VAD')\n",
    "\n",
    "        # base = pyplot.gca().transData\n",
    "        # rot = transforms.Affine2D().rotate_deg(90)\n",
    "        \n",
    "        for i,z in enumerate([0,95,195,295,395,495]):\n",
    "            try:\n",
    "                if np.isnan(vadu[z]):\n",
    "                    pass\n",
    "                else:\n",
    "                    circle = plt.Circle((vadu[z], vadv[z]), radius=1.5,color='k',zorder=30)\n",
    "                    ax3.add_patch(circle)\n",
    "                    label = ax3.annotate(i, xy=(vadu[z], vadv[z]), fontsize=12, ha=\"center\",va='center',color='white',zorder=30,weight=\"bold\")\n",
    "            except: pass\n",
    "        \n",
    "        \n",
    "        r=35\n",
    "        radar_namelist, radar_list = get_radar_from_aws_sam('KLBB', datetime.strptime(thefile.split('/')[-1].split('.')[0][29:],'%Y%m%d%H%M%S'), datetime.strptime(thefile.split('/')[-1].split('.')[0][29:],'%Y%m%d%H%M%S')+timedelta(minutes=5))\n",
    "        print(radar_namelist)\n",
    "        print(radar_list)\n",
    "        #scantime88d = datetime.strptime(str(radar_namelist)[40:57], '%Y%m%d_%H%M%S') #SAM DELETED THAT\n",
    "        swp_id = 0\n",
    "        radar88d = radar_list[0].extract_sweeps([0])\n",
    "        scantime88d = pyart.util.datetime_from_radar(radar88d)\n",
    "        # SAM ADDED THAT^^^\n",
    "        time_text = ' ' + str(radar88d.time['units'][-9:1])\n",
    "        title ='KLBB PPI ' +scantime88d.strftime(\"%m/%d/%y %H:%M\")+ ' UTC'\n",
    "        display = pyart.graph.RadarMapDisplay(radar88d)\n",
    "        ax2 = plt.subplot2grid((1,3),(0,1),projection=display.grid_projection)\n",
    "        display.plot_ppi_map('reflectivity', swp_id, cmap='pyart_HomeyerRainbow', ax=ax2, vmin=0., vmax=75., \n",
    "                             colorbar_label='Radar Reflectivity [dBZ]', title='Reflectivity',\n",
    "                             min_lon=getLocation(latmean, lonmean, 270, r)[1],\n",
    "                             max_lon=getLocation(latmean, lonmean, 90, r)[1],\n",
    "                             min_lat=getLocation(latmean, lonmean, 180, r)[0],\n",
    "                             max_lat=getLocation(latmean, lonmean, 0, r)[0])\n",
    "        \n",
    "        m = matplotlib.markers.MarkerStyle(marker='^')\n",
    "        m._transform = m.get_transform().rotate_deg(-bearmean)\n",
    "        ax2.plot(lonmean,latmean,color='k',marker=m,ms=20,transform=ccrs.PlateCarree())\n",
    "        m = matplotlib.markers.MarkerStyle(marker=3)\n",
    "        m._transform = m.get_transform().rotate_deg(-bearmean)\n",
    "        ax2.plot(lonmean,latmean,color='k',marker=m,ms=50,transform=ccrs.PlateCarree(),markeredgewidth=4)\n",
    "\n",
    "\n",
    "        fname=thefile.split('/')[-1].split('.')[0][:]\n",
    "        plt.suptitle(fname)#+' '+str(np.around(dfscan['Bearing[degrees]'].astype(float).var(),decimals=2))+' '+str(np.around(dfscan['Velocity[knots]'].astype(float).var(),decimals=2)))\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig(f\"/Users/juliabman/Desktop/alex_morton_hodos/{fname}.png\")\n",
    "        # plt.close()\n",
    "        # fig.clf()\n",
    "        # gc.collect()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1b4ec-13b3-43af-9fcf-73558ec3ec00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
